use crate::logger::Logger;
use crate::types::AIModelConfig;
use async_openai::{
    config::OpenAIConfig,
    types::{
        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessage,
        CreateChatCompletionRequestArgs, Role,
    },
    Client,
};
use futures_util::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::Instant;

/// åˆ†æè¿›åº¦çŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisProgress {
    pub status: String,           // "analyzing", "completed", "error"
    pub current_step: String,     // å½“å‰æ­¥éª¤æè¿°
    pub chunks_received: u32,     // å·²æ¥æ”¶çš„chunkæ•°é‡
    pub total_chars: usize,       // å·²æ¥æ”¶çš„æ€»å­—ç¬¦æ•°
    pub elapsed_seconds: f64,     // å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰
    pub error_message: Option<String>, // é”™è¯¯ä¿¡æ¯
}

/// å…¨å±€è¿›åº¦ç®¡ç†å™¨
pub struct ProgressManager {
    progress: Arc<Mutex<Option<AnalysisProgress>>>,
    cancelled: Arc<Mutex<bool>>,
}

// å…¨å±€è¿›åº¦ç®¡ç†å™¨å®ä¾‹
static GLOBAL_PROGRESS_MANAGER: std::sync::OnceLock<ProgressManager> = std::sync::OnceLock::new();

impl ProgressManager {
    pub fn new() -> Self {
        Self {
            progress: Arc::new(Mutex::new(None)),
            cancelled: Arc::new(Mutex::new(false)),
        }
    }

    pub fn start_analysis(&self) {
        // é‡ç½®å–æ¶ˆæ ‡å¿—
        let mut cancelled = self.cancelled.lock().unwrap();
        *cancelled = false;
        drop(cancelled);

        let mut progress = self.progress.lock().unwrap();
        *progress = Some(AnalysisProgress {
            status: "analyzing".to_string(),
            current_step: "å‡†å¤‡åˆ†æ...".to_string(),
            chunks_received: 0,
            total_chars: 0,
            elapsed_seconds: 0.0,
            error_message: None,
        });
    }

    pub fn update_step(&self, step: &str, start_time: Instant) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.current_step = step.to_string();
                p.elapsed_seconds = start_time.elapsed().as_secs_f64();
            }
        }
    }

    pub fn update_chunk(&self, chunks: u32, total_chars: usize, start_time: Instant) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.chunks_received = chunks;
                p.total_chars = total_chars;
                p.elapsed_seconds = start_time.elapsed().as_secs_f64();
            }
        }
    }

    pub fn complete_analysis(&self) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "completed".to_string();
                p.current_step = "åˆ†æå®Œæˆ".to_string();
            }
        }
    }

    pub fn error_analysis(&self, error: &str) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "error".to_string();
                p.current_step = "åˆ†æå¤±è´¥".to_string();
                p.error_message = Some(error.to_string());
            }
        }
    }

    pub fn get_progress(&self) -> Option<AnalysisProgress> {
        self.progress.lock().ok()?.clone()
    }

    pub fn clear_progress(&self) {
        let mut progress = self.progress.lock().unwrap();
        *progress = None;
    }

    pub fn cancel_analysis(&self) {
        let mut cancelled = self.cancelled.lock().unwrap();
        *cancelled = true;
        drop(cancelled);

        // æ›´æ–°è¿›åº¦çŠ¶æ€ä¸ºå·²å–æ¶ˆ
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "cancelled".to_string();
                p.current_step = "åˆ†æå·²å–æ¶ˆ".to_string();
            }
        }
    }

    pub fn is_cancelled(&self) -> bool {
        self.cancelled.lock().map(|guard| *guard).unwrap_or(false)
    }
}

/// è·å–å…¨å±€è¿›åº¦ç®¡ç†å™¨
pub fn get_global_progress_manager() -> &'static ProgressManager {
    GLOBAL_PROGRESS_MANAGER.get_or_init(|| ProgressManager::new())
}

/// AI æœåŠ¡æä¾›å•†é…ç½®ï¼ˆåŠ¨æ€é…ç½®ï¼‰
#[derive(Debug, Clone)]
pub struct AIProvider {
    pub name: String,
    pub base_url: String,
    pub api_key: String,
    pub default_model: String,
}

impl AIProvider {
    pub fn get_api_key(&self) -> &str {
        &self.api_key
    }

    pub fn get_base_url(&self) -> &str {
        &self.base_url
    }

    pub fn get_default_model(&self) -> &str {
        &self.default_model
    }
}

// ç§»é™¤äº†ä¼ ç»Ÿè¯æ±‡åˆ†æç›¸å…³çš„ç»“æ„ä½“ï¼Œåªä¿ç•™è‡ªç„¶æ‹¼è¯»åˆ†æ

/// ç®€å•çš„èŠå¤©æ¶ˆæ¯
#[derive(Debug, Serialize, Deserialize)]
pub struct SimpleChatMessage {
    pub role: String,
    pub content: String,
}

/// èŠå¤©å®Œæˆè¯·æ±‚
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionRequest {
    pub model: String,
    pub messages: Vec<SimpleChatMessage>,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f32>,
}

/// èŠå¤©å®Œæˆå“åº”
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionChoice {
    pub message: SimpleChatMessage,
    pub finish_reason: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionResponse {
    pub choices: Vec<ChatCompletionChoice>,
    pub usage: Option<HashMap<String, u32>>,
}

/// CSV è®°å½•ç»“æ„ï¼ˆç”¨äºæ‰¹é‡åˆ†æï¼‰
#[derive(Debug, Deserialize)]
struct CsvPhonicsRecord {
    word: String,
    frequency: i32,
    chinese_translation: String,
    pos_abbreviation: String,
    pos_english: String,
    pos_chinese: String,
    ipa: String,
    syllables: String,
    phonics_rule: String,
    analysis_explanation: String,
}

impl CsvPhonicsRecord {
    fn into_phonics_word(self) -> PhonicsWord {
        PhonicsWord {
            word: self.word,
            frequency: self.frequency,
            chinese_translation: self.chinese_translation,
            pos_abbreviation: self.pos_abbreviation,
            pos_english: self.pos_english,
            pos_chinese: self.pos_chinese,
            ipa: self.ipa,
            syllables: self.syllables,
            phonics_rule: self.phonics_rule,
            analysis_explanation: self.analysis_explanation,
        }
    }
}

/// é€šç”¨ AI æœåŠ¡
pub struct AIService {
    provider: AIProvider,
    client: Client<OpenAIConfig>,
}

impl AIService {
    /// ä½¿ç”¨æŒ‡å®šæä¾›å•†åˆ›å»º AI æœåŠ¡
    pub fn new(provider: AIProvider) -> Self {
        let config = OpenAIConfig::new()
            .with_api_key(&provider.api_key)
            .with_api_base(&provider.base_url);

        Self {
            provider,
            client: Client::with_config(config),
        }
    }

    /// ä»æ•°æ®åº“æ¨¡å‹é…ç½®åˆ›å»º AI æœåŠ¡
    pub fn from_model_config(
        model_config: &AIModelConfig,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let provider = AIProvider {
            name: model_config.provider.name.clone(),
            base_url: model_config.provider.base_url.clone(),
            api_key: model_config.provider.api_key.clone(),
            default_model: model_config.model_id.clone(),
        };

        Ok(Self::new(provider))
    }

    /// è·å–åˆ†æè¿›åº¦
    pub fn get_analysis_progress(&self) -> Option<AnalysisProgress> {
        get_global_progress_manager().get_progress()
    }

    /// æ¸…é™¤åˆ†æè¿›åº¦
    pub fn clear_analysis_progress(&self) {
        get_global_progress_manager().clear_progress()
    }

    // ç§»é™¤äº†ä¼ ç»Ÿè¯æ±‡åˆ†ææ–¹æ³•ï¼Œåªä¿ç•™è‡ªç„¶æ‹¼è¯»åˆ†æ

    /// æå–å•è¯åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡åˆ†æçš„ç¬¬ä¸€æ­¥ï¼‰
    pub async fn extract_words(
        &self,
        text: &str,
        logger: &Logger,
    ) -> Result<crate::types::word_analysis::WordExtractionResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting word extraction for text: {}",
                if text.len() > 100 { &text[..100] } else { text }
            ),
        );

        // è¯»å–å•è¯æå–æç¤ºè¯
        let extraction_prompt = include_str!("prompts/word_extraction_agent.md");

        // æ„å»ºå®Œæ•´çš„æç¤ºè¯
        let system_message = extraction_prompt.replace("{original_text}", text);

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Built extraction prompt: {} chars",
                system_message.len()
            ),
        );

        // æ„å»ºè¯·æ±‚ï¼ˆä½¿ç”¨å° max_tokensï¼Œå› ä¸ºåªéœ€è¦å•è¯åˆ—è¡¨ï¼‰
        let request = CreateChatCompletionRequestArgs::default()
            .model(self.provider.get_default_model())
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: system_message,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(2000u16)  // é™åˆ¶è¾“å‡ºé•¿åº¦
            .temperature(0.1)    // ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
            .stream(false)        // éæµå¼ï¼Œå¿«é€Ÿè·å–ç»“æœ
            .build()?;

        logger.info("AI_SERVICE", "ğŸ“¤ Sending word extraction request...");

        // å‘é€è¯·æ±‚
        let response = self.client.chat().create(request).await
            .map_err(|e| {
                logger.info("AI_SERVICE", &format!("âŒ Word extraction failed: {}", e));
                format!("Word extraction failed: {}", e)
            })?;

        // æå–å“åº”å†…å®¹
        let content = response.choices.first()
            .and_then(|choice| choice.message.content.as_ref())
            .ok_or("No content in word extraction response")?;

        logger.info(
            "AI_SERVICE",
            &format!(
                "âœ… Word extraction successful - Response length: {} chars",
                content.len()
            ),
        );

        // è§£æ CSV å“åº”
        let words = self.parse_csv_response(content, &logger)?;

        let total_count = words.len();
        let unique_count = words.len();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“Š Extracted {} unique words (total occurrences: {}) in {:?}",
                unique_count,
                total_count,
                start_time.elapsed()
            ),
        );

        Ok(crate::types::word_analysis::WordExtractionResult {
            words,
            total_count,
            unique_count,
        })
    }

    /// æ‰¹é‡åˆ†æå•è¯ï¼ˆç”¨äºæ‰¹é‡åˆ†æçš„ç¬¬äºŒæ­¥ï¼‰
    pub async fn analyze_words_batch(
        &self,
        words: Vec<String>,
        batch_index: usize,
        total_batches: usize,
        logger: &Logger,
    ) -> Result<Vec<PhonicsWord>, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();
        
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting batch analysis: batch {}/{}, {} words",
                batch_index + 1,
                total_batches,
                words.len()
            ),
        );
        
        // è¯»å–æ‰¹é‡è‡ªç„¶æ‹¼è¯»åˆ†ææç¤ºè¯ï¼ˆCSVæ ¼å¼ï¼‰
        let phonics_prompt_template = include_str!("prompts/batch_phonics_agent.md");
        
        // æ„å»ºæ‰¹é‡åˆ†ææç¤ºè¯
        let words_list = words.join(", ");
        
        let batch_prompt = phonics_prompt_template.replace("{word_list}", &words_list);
        
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Built batch prompt: {} chars",
                batch_prompt.len()
            ),
        );
        
        // æ„å»ºè¯·æ±‚
        let request = CreateChatCompletionRequestArgs::default()
            .model(self.provider.get_default_model())
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: batch_prompt,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(8000u16)  // æ¯æ‰¹ 5 ä¸ªå•è¯
            .temperature(0.1)
            .stream(false)        // ä½¿ç”¨éæµå¼è¾“å‡ºï¼Œç›´æ¥è·å–å®Œæ•´ CSV
            .build()?;
        
        logger.info("AI_SERVICE", "ğŸ“¤ Sending batch analysis request...");
        
        // å‘é€è¯·æ±‚å¹¶è·å–å®Œæ•´å“åº”
        let response = self.client.chat().create(request).await
            .map_err(|e| {
                logger.info("AI_SERVICE", &format!("âŒ Batch analysis failed: {}", e));
                format!("Batch analysis failed: {}", e)
            })?;
        
        // æå–å“åº”å†…å®¹
        let content = response.choices.first()
            .and_then(|choice| choice.message.content.as_ref())
            .ok_or("No content in batch analysis response")?;
        
        logger.info(
            "AI_SERVICE",
            &format!(
                "âœ… Batch analysis successful - Response length: {} chars in {:?}",
                content.len(),
                start_time.elapsed()
            ),
        );
        
        // è§£æ CSV å“åº”
        let result_words = self.parse_batch_csv_response(content, logger)?;
        
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“Š Batch {}/{} completed: {} words analyzed in {:?}",
                batch_index + 1,
                total_batches,
                result_words.len(),
                start_time.elapsed()
            ),
        );
        
        Ok(result_words)
    }

    /// è‡ªç„¶æ‹¼è¯»åˆ†æ
    pub async fn analyze_phonics(
        &self,
        text: &str,
        model_name: Option<&str>,
        max_tokens: Option<u32>,
        temperature: Option<f32>,
        extraction_mode: &str,
        logger: &Logger,
    ) -> Result<PhonicsAnalysisResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();

        // å¯åŠ¨è¿›åº¦è·Ÿè¸ª
        let progress_manager = get_global_progress_manager();
        progress_manager.start_analysis();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting phonics analysis for text: {}",
                if text.len() > 100 { &text[..100] } else { text }
            ),
        );

        // æ­¥éª¤1: è¯»å–æç¤ºè¯æ¨¡æ¿
        progress_manager.update_step("è¯»å–æç¤ºè¯æ¨¡æ¿...", start_time);
        let step1_start = std::time::Instant::now();
        let prompt_template = include_str!("prompts/phonics_agent.md");
        let step1_duration = step1_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Step 1 - Loaded prompt template: {} chars in {:?}",
                prompt_template.len(),
                step1_duration
            ),
        );

        // æ­¥éª¤2: æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆæ›¿æ¢å ä½ç¬¦ï¼‰
        progress_manager.update_step("æ„å»ºåˆ†ææç¤ºè¯...", start_time);
        let step2_start = std::time::Instant::now();

        // æ ¹æ®æå–æ¨¡å¼ç”Ÿæˆé¢å¤–çš„é¢„å¤„ç†æ­¥éª¤
        let additional_steps = if extraction_mode == "focus" {
            r#"4. è¿‡æ»¤ç®€å•è¯æ±‡ï¼šæ’é™¤ä»¥ä¸‹ç±»å‹çš„ç®€å•è¯æ±‡ï¼Œè¿™äº›è¯æ±‡ä¸é€‚åˆä½œä¸ºå­¦ä¹ é‡ç‚¹
   - å† è¯ï¼ša, an, the
   - åŸºç¡€ä»£è¯ï¼šI, you, he, she, it, we, they, me, him, her, us, them
   - åŸºç¡€beåŠ¨è¯ï¼šam, is, are, was, were, be, been, being
   - åŸºç¡€åŠ©åŠ¨è¯ï¼šdo, does, did, have, has, had, will, would, can, could, should, shall, may, might, must
   - åŸºç¡€ä»‹è¯ï¼šin, on, at, to, for, of, with, by, from, up, out, off, over, under
   - åŸºç¡€è¿è¯ï¼šand, or, but, so, if, when, then, than, as
   - åŸºç¡€å‰¯è¯ï¼šnot, no, yes, very, too, also, only, just, now, here, there
   - å•å­—æ¯è¯ï¼ša, I
   - è¿‡äºç®€å•çš„æ•°å­—è¯ï¼šone, two, three, four, five, six, seven, eight, nine, ten

5. é‡ç‚¹å…³æ³¨ï¼šä¼˜å…ˆåˆ†æå…·æœ‰å­¦ä¹ ä»·å€¼çš„å®è¯ï¼ˆåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯ï¼‰å’Œä¸­ç­‰éš¾åº¦ä»¥ä¸Šçš„åŠŸèƒ½è¯"#
        } else {
            // all æ¨¡å¼ä¸‹ä¸æ·»åŠ é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤
            ""
        };

        let system_message = prompt_template
            .replace("{original_text}", text)
            .replace("{additional_text_preprocessing_steps}", additional_steps);

        let step2_duration = step2_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“ Step 2 - Built complete prompt with {} mode: {} chars in {:?}",
                extraction_mode,
                system_message.len(),
                step2_duration
            ),
        );



        // æ­¥éª¤3: å‡†å¤‡æ¨¡å‹å‚æ•°
        progress_manager.update_step("å‡†å¤‡AIæ¨¡å‹å‚æ•°...", start_time);
        let step3_start = std::time::Instant::now();
        let actual_model_name = model_name.unwrap_or(self.provider.get_default_model());
        let final_max_tokens = max_tokens.unwrap_or(8000);
        let final_temperature = temperature.unwrap_or(0.1);
        let step3_duration = step3_start.elapsed();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”§ Step 3 - Model parameters prepared in {:?}:",
                step3_duration
            ),
        );
        logger.info("AI_SERVICE", &format!("   ğŸ“Š Model: {}", actual_model_name));
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ¯ Max tokens: {}", final_max_tokens),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸŒ¡ï¸  Temperature: {}", final_temperature),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸŒ Base URL: {}", self.provider.base_url),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key length: {}", self.provider.api_key.len()),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key preview: {}...",
                if self.provider.api_key.len() > 10 {
                    &self.provider.api_key[..10]
                } else {
                    &self.provider.api_key
                }
            ),
        );

        // æ­¥éª¤4: æ„å»º AI è¯·æ±‚ï¼ˆä¸ä½¿ç”¨æµå¼è¾“å‡ºï¼Œä¸€æ¬¡æ€§è·å–å®Œæ•´ç»“æœï¼‰
        progress_manager.update_step("æ„å»ºAIè¯·æ±‚...", start_time);
        let step4_start = std::time::Instant::now();
        let request = CreateChatCompletionRequestArgs::default()
            .model(actual_model_name)
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: system_message,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(final_max_tokens.min(65535) as u16)
            .temperature(final_temperature)
            .stream(false) // ä¸ä½¿ç”¨æµå¼è¾“å‡ºï¼Œä¸€æ¬¡æ€§è·å–å®Œæ•´ç»“æœ
            .build()?;
        let step4_duration = step4_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”¨ Step 4 - Built AI streaming request in {:?}",
                step4_duration
            ),
        );

        // æ­¥éª¤5: å‘é€ AI API æµå¼è¯·æ±‚
        progress_manager.update_step("è¿æ¥AIæœåŠ¡...", start_time);
        let step5_start = std::time::Instant::now();
        logger.info(
            "AI_SERVICE",
            "ğŸŒ Step 5 - Sending streaming request to AI API...",
        );
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¤ Request details: {} tokens max, temp {}, to {}",
                final_max_tokens, final_temperature, self.provider.base_url
            ),
        );

        let response = self.client.chat().create(request).await
            .map_err(|e| {
                logger.info("AI_SERVICE", &format!("âŒ Request failed: {}", e));
                format!("Request failed: {}", e)
            })?;
        let step5_duration = step5_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¡ Step 5 - Response received in {:?}",
                step5_duration
            ),
        );

        // æ­¥éª¤6: æå–å“åº”å†…å®¹
        let step6_start = std::time::Instant::now();
        let content = response.choices.first()
            .and_then(|choice| choice.message.content.as_ref())
            .ok_or("No content in word extraction response")?;

        let step6_duration = step6_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "âœ… Step 6 - Extracted content in {:?}: {} chars",
                step6_duration,
                content.len()
            ),
        );

        // æ­¥éª¤7: è§£æ JSON å“åº”
        let step7_start = std::time::Instant::now();
        match self.parse_phonics_json(&content) {
            Ok(result) => {
                let step7_duration = step7_start.elapsed();
                let total_duration = start_time.elapsed();
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "ğŸ‰ Step 7 - Successfully parsed {} phonics entries in {:?}",
                        result.words.len(),
                        step7_duration
                    ),
                );
                logger.info(
                    "AI_SERVICE",
                    &format!("â±ï¸  Total analysis time: {:?}", total_duration),
                );

                // æ ‡è®°åˆ†æå®Œæˆ
                progress_manager.complete_analysis();

                Ok(result)
            }
            Err(e) => {
                let step7_duration = step7_start.elapsed();
                let total_duration = start_time.elapsed();
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "âŒ Step 7 - JSON parsing failed in {:?}: {}",
                        step7_duration, e
                    ),
                );
                logger.info(
                    "AI_SERVICE",
                    &format!("ğŸ“„ Full AI response for debugging: {}", content),
                );
                logger.info(
                    "AI_SERVICE",
                    &format!("â±ï¸  Total time before failure: {:?}", total_duration),
                );

                // æ ‡è®°åˆ†æå¤±è´¥
                let error_msg = format!("Failed to parse phonics analysis: JSON parsing error: {}", e);
                progress_manager.error_analysis(&error_msg);

                Err(error_msg.into())
            }
        }
    }

    /// ç”Ÿæˆå­¦ä¹ è®¡åˆ’æ—¥ç¨‹è§„åˆ’
    pub async fn generate_study_plan_schedule(
        &self,
        params: crate::types::study::StudyPlanAIParams,
        model_config: &AIModelConfig,
        logger: &Logger,
    ) -> Result<crate::types::study::StudyPlanAIResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();

        // å¯åŠ¨è¿›åº¦è·Ÿè¸ª
        let progress_manager = get_global_progress_manager();
        progress_manager.start_analysis();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting study plan schedule generation: {} words, {} days, {} intensity",
                params.total_words, params.period_days, params.intensity_level
            ),
        );

        // æ­¥éª¤1: è¯»å–æç¤ºè¯æ¨¡æ¿
        progress_manager.update_step("è¯»å–å­¦ä¹ è®¡åˆ’æç¤ºè¯æ¨¡æ¿...", start_time);
        let step1_start = std::time::Instant::now();
        let prompt_template = include_str!("prompts/study_plan_agent.md");
        let step1_duration = step1_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Step 1 - Loaded study plan prompt template: {} chars in {:?}",
                prompt_template.len(),
                step1_duration
            ),
        );

        // æ­¥éª¤2: æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆæ›¿æ¢å ä½ç¬¦ï¼‰
        progress_manager.update_step("æ„å»ºå­¦ä¹ è®¡åˆ’è§„åˆ’æç¤ºè¯...", start_time);
        let step2_start = std::time::Instant::now();

        // å°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        let word_list_json = match serde_json::to_string_pretty(&params.word_list) {
            Ok(json) => json,
            Err(e) => {
                let error_msg = format!("Failed to serialize word list: {}", e);
                progress_manager.error_analysis(&error_msg);
                return Err(error_msg.into());
            }
        };

        // æ›¿æ¢æç¤ºè¯ä¸­çš„å ä½ç¬¦
        let full_prompt = prompt_template
            .replace("{intensity_level}", &params.intensity_level)
            .replace("{total_words}", &params.total_words.to_string())
            .replace("{period_days}", &params.period_days.to_string())
            .replace("{review_frequency}", &params.review_frequency.to_string())
            .replace("{start_date}", &params.start_date)
            .replace("{word_list}", &word_list_json);

        let step2_duration = step2_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“ Step 2 - Built full prompt: {} chars in {:?}",
                full_prompt.len(),
                step2_duration
            ),
        );

        // è¾“å‡ºå®Œæ•´æç¤ºè¯ç”¨äºè°ƒè¯•
        logger.info(
            "AI_SERVICE",
            &format!("ğŸ“ Full prompt content:\n{}", full_prompt),
        );

        // æ­¥éª¤3: å‡†å¤‡APIè¯·æ±‚å‚æ•°
        progress_manager.update_step("å‡†å¤‡AIè¯·æ±‚å‚æ•°...", start_time);
        let step3_start = std::time::Instant::now();

        let model_name = model_config.model_id.as_str();
        let max_tokens = model_config.max_tokens.unwrap_or(4000) as u32;
        let temperature = model_config.temperature.unwrap_or(0.3) as f32;

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”§ Step 3 - API params: model={}, max_tokens={}, temperature={}",
                model_name, max_tokens, temperature
            ),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”— Base URL: {}", self.provider.base_url),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key preview: {}...",
                if self.provider.api_key.len() > 10 {
                    &self.provider.api_key[..10]
                } else {
                    &self.provider.api_key
                }
            ),
        );

        // æµ‹è¯•ç½‘ç»œè¿æ¥
        logger.info(
            "AI_SERVICE",
            "ğŸŒ Testing network connectivity to OpenRouter...",
        );

        let step3_duration = step3_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("âš™ï¸  Step 3 completed in {:?}", step3_duration),
        );

        // æ­¥éª¤4: åˆ›å»ºèŠå¤©è¯·æ±‚
        progress_manager.update_step("åˆ›å»ºAIèŠå¤©è¯·æ±‚...", start_time);
        let step4_start = std::time::Instant::now();

        let request = CreateChatCompletionRequestArgs::default()
            .model(model_name)
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: full_prompt,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(max_tokens.min(65535) as u16)
            .temperature(temperature)
            .stream(true)
            .build()?;

        let step4_duration = step4_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("ğŸ“¤ Step 4 - Created chat request in {:?}", step4_duration),
        );

        // æ­¥éª¤5: å‘é€è¯·æ±‚å¹¶å»ºç«‹æµè¿æ¥
        progress_manager.update_step("å‘é€AIè¯·æ±‚...", start_time);
        let step5_start = std::time::Instant::now();

        let stream = self.client.chat().create_stream(request).await;

        let step5_duration = step5_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("ğŸŒ Step 5 - Sent request in {:?}", step5_duration),
        );

        // æ­¥éª¤6: å¤„ç†æµå¼å“åº”
        let step6_start = std::time::Instant::now();
        let mut stream = match stream {
            Ok(s) => {
                logger.info(
                    "AI_SERVICE",
                    "âœ… Successfully established stream connection for study plan",
                );
                s
            }
            Err(e) => {
                let total_duration = start_time.elapsed();
                let error_msg = format!("Failed to establish stream connection: {}", e);
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "âŒ Stream connection failed after {:?}: {}",
                        total_duration, error_msg
                    ),
                );
                progress_manager.error_analysis(&error_msg);
                return Err(error_msg.into());
            }
        };

        // æ­¥éª¤7: æ”¶é›†æµå¼å“åº”å†…å®¹
        progress_manager.update_step("æ¥æ”¶AIå­¦ä¹ è®¡åˆ’è§„åˆ’ç»“æœ...", start_time);
        logger.info(
            "AI_SERVICE",
            "ğŸ“¥ Step 7 - Starting to receive study plan stream response",
        );

        let mut full_content = String::new();
        let mut chunk_count = 0;
        let mut last_log_time = std::time::Instant::now();

        while let Some(result) = stream.next().await {
            // æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
            if progress_manager.is_cancelled() {
                logger.info("AI_SERVICE", "ğŸš« Study plan generation cancelled by user");
                // è¿”å›ä¸€ä¸ªç©ºçš„ç»“æœè€Œä¸æ˜¯é”™è¯¯ï¼Œä¸è‡ªç„¶æ‹¼è¯»åˆ†æä¿æŒä¸€è‡´
                return Ok(crate::types::study::StudyPlanAIResult {
                    plan_metadata: crate::types::study::StudyPlanMetadata {
                        total_words: 0,
                        study_period_days: 0,
                        intensity_level: "".to_string(),
                        review_frequency: 0,
                        plan_type: "".to_string(),
                        start_date: "".to_string(),
                        end_date: "".to_string(),
                    },
                    daily_plans: vec![],
                });
            }

            match result {
                Ok(response) => {
                    chunk_count += 1;

                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta.content {
                            full_content.push_str(delta);

                            // æ›´æ–°è¿›åº¦ç®¡ç†å™¨
                            progress_manager.update_chunk(chunk_count, full_content.len(), start_time);

                            // æ¯5ç§’è®°å½•ä¸€æ¬¡è¿›åº¦
                            if last_log_time.elapsed().as_secs() >= 5 {
                                logger.info(
                                    "AI_SERVICE",
                                    &format!(
                                        "ğŸ“¥ Received {} chunks, total: {} chars",
                                        chunk_count,
                                        full_content.len()
                                    ),
                                );
                                last_log_time = std::time::Instant::now();
                            }
                        }
                    }
                }
                Err(e) => {
                    let total_duration = start_time.elapsed();
                    let error_msg = format!("Stream error: {}", e);
                    progress_manager.error_analysis(&error_msg);

                    // è¯¦ç»†çš„ç½‘ç»œé”™è¯¯è¯Šæ–­
                    logger.info(
                        "AI_SERVICE",
                        &format!("âŒ Stream error after {:?}: {}", total_duration, e),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("ğŸ” Network Error Details:"),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ“¡ Target URL: {}", self.provider.base_url),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ”‘ API Key Length: {} chars", self.provider.api_key.len()),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   â±ï¸  Request Duration: {:?}", total_duration),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ¯ Model: {}", model_name),
                    );

                    // æ£€æŸ¥é”™è¯¯ç±»å‹
                    let error_str = e.to_string();
                    if error_str.contains("dns") || error_str.contains("resolve") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Possible DNS resolution issue. Check internet connection.",
                        );
                    } else if error_str.contains("timeout") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Request timeout. The API might be slow or overloaded.",
                        );
                    } else if error_str.contains("connection") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Connection issue. Check firewall or proxy settings.",
                        );
                    } else if error_str.contains("401") || error_str.contains("unauthorized") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Authentication issue. Check API key validity.",
                        );
                    } else if error_str.contains("429") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Rate limit exceeded. Wait before retrying.",
                        );
                    } else if error_str.contains("500") || error_str.contains("502") || error_str.contains("503") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Server error. The API service might be temporarily unavailable.",
                        );
                    }

                    return Err(error_msg.into());
                }
            }
        }

        let step6_duration = step6_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¥ Step 6 - Received {} chunks, {} chars in {:?}",
                chunk_count,
                full_content.len(),
                step6_duration
            ),
        );

        // æ­¥éª¤8: éªŒè¯å“åº”å†…å®¹
        if !full_content.is_empty() {
            logger.info(
                "AI_SERVICE",
                &format!(
                    "ğŸ“„ Step 8 - Response content length: {} chars",
                    full_content.len()
                ),
            );

            // è¾“å‡ºAIçš„åŸå§‹è¿”å›å†…å®¹ç”¨äºè°ƒè¯•
            logger.info(
                "AI_SERVICE",
                &format!("ğŸ“„ AIåŸå§‹è¿”å›å†…å®¹:\n{}", full_content),
            );

            // æ£€æŸ¥æ˜¯å¦åŒ…å«JSONç»“æ„
            if !full_content.contains("plan_metadata") || !full_content.contains("daily_plans") {
                logger.info(
                    "AI_SERVICE",
                    "âš ï¸  Response does not contain complete study plan JSON structure",
                );
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "ğŸ“„ First 1000 chars: {}",
                        full_content.chars().take(1000).collect::<String>()
                    ),
                );
            }

            // æ­¥éª¤9: è§£æ JSON å“åº”
            let step9_start = std::time::Instant::now();
            match self.parse_study_plan_json(&full_content) {
                Ok(result) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "âœ… Step 9 - Successfully parsed study plan with {} daily plans in {:?}",
                            result.daily_plans.len(), step9_duration
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total study plan generation time: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå®Œæˆ
                    progress_manager.complete_analysis();

                    Ok(result)
                }
                Err(e) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "âŒ Step 9 - Study plan JSON parsing failed in {:?}: {}",
                            step9_duration, e
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("ğŸ“„ Full AI response for debugging: {}", full_content),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total time before failure: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå¤±è´¥
                    let error_msg = format!("Failed to parse study plan: JSON parsing error: {}", e);
                    progress_manager.error_analysis(&error_msg);

                    Err(error_msg.into())
                }
            }
        } else {
            let total_duration = start_time.elapsed();
            logger.info(
                "AI_SERVICE",
                &format!(
                    "âŒ No content received from study plan streaming response after {:?}",
                    total_duration
                ),
            );
            let error_msg = "No content received from streaming response";
            progress_manager.error_analysis(error_msg);
            Err(error_msg.into())
        }
    }

    /// è§£æå­¦ä¹ è®¡åˆ’è§„åˆ’çš„ JSON å“åº”
    fn parse_study_plan_json(
        &self,
        json_content: &str,
    ) -> Result<crate::types::study::StudyPlanAIResult, Box<dyn std::error::Error>> {
        // æå–JSONéƒ¨åˆ†
        let json_str = self.extract_json_from_response(json_content);

        // æ¸…ç†JSONè¯­æ³•é”™è¯¯
        let cleaned_json = self.clean_json_syntax(&json_str);

        // å…ˆè§£æä¸ºé€šç”¨çš„JSONå€¼ï¼Œç„¶åè¿›è¡Œå­—æ®µåè½¬æ¢
        let ai_json: serde_json::Value = serde_json::from_str(&cleaned_json)?;

        // è½¬æ¢å­—æ®µåä»ä¸‹åˆ’çº¿åˆ°é©¼å³°å‘½å
        let converted_json = self.convert_study_plan_field_names(ai_json)?;

        // è§£æä¸ºæœ€ç»ˆçš„ç»“æ„ä½“
        let result: crate::types::study::StudyPlanAIResult = serde_json::from_value(converted_json)?;

        // éªŒè¯ç»“æœ
        if result.daily_plans.is_empty() {
            return Err("No daily plans found in JSON response".into());
        }

        Ok(result)
    }

    /// æ¸…ç†JSONè¯­æ³•é”™è¯¯
    fn clean_json_syntax(&self, json_str: &str) -> String {
        let mut cleaned = json_str.to_string();

        // ä¿®å¤JavaScriptå¯¹è±¡è¯­æ³•ä¸ºJSONè¯­æ³•
        // 1. ä¿®å¤æ— å¼•å·çš„å±æ€§å
        let property_replacements = [
            ("word_id:", r#""word_id":"#),
            ("word:", r#""word":"#),
            ("wordbook_id:", r#""wordbook_id":"#),
            ("is_review:", r#""is_review":"#),
            ("review_count:", r#""review_count":"#),
            ("priority:", r#""priority":"#),
            ("difficulty_level:", r#""difficulty_level":"#),
        ];

        for (pattern, replacement) in property_replacements.iter() {
            // åªæ›¿æ¢åœ¨å¯¹è±¡å¼€å§‹ä½ç½®çš„å±æ€§åï¼ˆå‰é¢æœ‰ç©ºç™½å­—ç¬¦ã€{æˆ–,ï¼‰
            let lines: Vec<&str> = cleaned.lines().collect();
            let mut new_lines = Vec::new();

            for line in lines {
                let trimmed = line.trim_start();
                if trimmed.starts_with(pattern) && !trimmed.starts_with(&format!("\"{}\"", &pattern[..pattern.len()-1])) {
                    // æ›¿æ¢æ— å¼•å·çš„å±æ€§å
                    let new_line = line.replace(pattern, replacement);
                    new_lines.push(new_line);
                } else {
                    new_lines.push(line.to_string());
                }
            }
            cleaned = new_lines.join("\n");
        }

        // 2. ä¿®å¤æ— å¼•å·çš„æ—¥æœŸå€¼ï¼ˆå¦‚ "date": 2025-08-27 åº”è¯¥æ˜¯ "date": "2025-08-27"ï¼‰
        // æŸ¥æ‰¾ "date": åé¢è·Ÿç€æ— å¼•å·çš„æ—¥æœŸ
        if cleaned.find(r#""date": 2025-08-27"#).is_some() {
            cleaned = cleaned.replace(r#""date": 2025-08-27"#, r#""date": "2025-08-27""#);
        }

        cleaned
    }

    /// è½¬æ¢å­¦ä¹ è®¡åˆ’JSONçš„å­—æ®µåä»ä¸‹åˆ’çº¿åˆ°é©¼å³°å‘½å
    fn convert_study_plan_field_names(
        &self,
        mut json: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = json {
            // è½¬æ¢é¡¶çº§å­—æ®µ
            if let Some(plan_metadata) = obj.remove("plan_metadata") {
                obj.insert("planMetadata".to_string(), self.convert_metadata_fields(plan_metadata)?);
            }
            if let Some(daily_plans) = obj.remove("daily_plans") {
                obj.insert("dailyPlans".to_string(), self.convert_daily_plans_fields(daily_plans)?);
            }
        }
        Ok(json)
    }

    /// è½¬æ¢å…ƒæ•°æ®å­—æ®µå
    fn convert_metadata_fields(
        &self,
        mut metadata: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = metadata {
            // è½¬æ¢å­—æ®µå
            if let Some(val) = obj.remove("total_words") {
                obj.insert("totalWords".to_string(), val);
            }
            if let Some(val) = obj.remove("study_period_days") {
                obj.insert("studyPeriodDays".to_string(), val);
            }
            if let Some(val) = obj.remove("intensity_level") {
                obj.insert("intensityLevel".to_string(), val);
            }
            if let Some(val) = obj.remove("review_frequency") {
                obj.insert("reviewFrequency".to_string(), val);
            }
            if let Some(val) = obj.remove("plan_type") {
                obj.insert("planType".to_string(), val);
            }
            if let Some(val) = obj.remove("start_date") {
                obj.insert("startDate".to_string(), val);
            }
            if let Some(val) = obj.remove("end_date") {
                obj.insert("endDate".to_string(), val);
            }
        }
        Ok(metadata)
    }

    /// è½¬æ¢æ¯æ—¥è®¡åˆ’å­—æ®µå
    fn convert_daily_plans_fields(
        &self,
        daily_plans: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Array(plans) = daily_plans {
            let converted_plans: Result<Vec<serde_json::Value>, _> = plans
                .into_iter()
                .map(|plan| self.convert_daily_plan_fields(plan))
                .collect();
            Ok(serde_json::Value::Array(converted_plans?))
        } else {
            Ok(daily_plans)
        }
    }

    /// è½¬æ¢å•ä¸ªæ¯æ—¥è®¡åˆ’çš„å­—æ®µå
    fn convert_daily_plan_fields(
        &self,
        mut plan: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = plan {
            // è½¬æ¢wordsæ•°ç»„
            if let Some(words) = obj.remove("words") {
                if let serde_json::Value::Array(word_list) = words {
                    let converted_words: Result<Vec<serde_json::Value>, _> = word_list
                        .into_iter()
                        .map(|word| self.convert_word_fields(word))
                        .collect();
                    obj.insert("words".to_string(), serde_json::Value::Array(converted_words?));
                }
            }
        }
        Ok(plan)
    }

    /// è½¬æ¢å•è¯å­—æ®µå
    fn convert_word_fields(
        &self,
        mut word: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = word {
            // è½¬æ¢å­—æ®µå
            if let Some(val) = obj.remove("word_id") {
                obj.insert("wordId".to_string(), val);
            }
            if let Some(val) = obj.remove("wordbook_id") {
                obj.insert("wordbookId".to_string(), val);
            }
            if let Some(val) = obj.remove("is_review") {
                obj.insert("isReview".to_string(), val);
            }
            if let Some(val) = obj.remove("review_count") {
                obj.insert("reviewCount".to_string(), val);
            }
            if let Some(val) = obj.remove("difficulty_level") {
                obj.insert("difficultyLevel".to_string(), val);
            }
        }
        Ok(word)
    }

    /// è§£æè‡ªç„¶æ‹¼è¯»åˆ†æçš„ JSON å“åº”
    fn parse_phonics_json(
        &self,
        json_content: &str,
    ) -> Result<PhonicsAnalysisResult, Box<dyn std::error::Error>> {
        // æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤å¯èƒ½çš„å‰åæ–‡æœ¬ï¼‰
        let json_str = self.extract_json_from_response(json_content);

        // è§£æJSON
        let json_response: JsonPhonicsResponse = serde_json::from_str(&json_str)
            .map_err(|e| format!("JSON parsing error: {}", e))?;

        // è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼
        let words: Vec<PhonicsWord> = json_response.words
            .into_iter()
            .filter(|w| !w.word.is_empty()) // è¿‡æ»¤ç©ºå•è¯
            .map(|w| w.into())
            .collect();

        if words.is_empty() {
            return Err("No valid words found in JSON response".into());
        }

        Ok(PhonicsAnalysisResult { words })
    }

    /// ç®€å•çš„å¯¹è¯å®Œæˆï¼ˆç”¨äºæ¨¡å‹æµ‹è¯•ï¼‰
    pub async fn chat_completion(
        &self,
        prompt: &str,
        max_tokens: Option<u32>,
        temperature: Option<f32>,
        logger: &Logger,
    ) -> Result<String, Box<dyn std::error::Error>> {
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting chat completion for prompt: {}",
                if prompt.len() > 100 { &prompt[..100] } else { prompt }
            ),
        );
        
        let model_name = self.provider.get_default_model();
        let final_max_tokens = max_tokens.unwrap_or(100);
        let final_temperature = temperature.unwrap_or(0.7);
        
        logger.info("AI_SERVICE", &format!(
            "Chat completion parameters - model: {}, max_tokens: {}, temperature: {}",
            model_name, final_max_tokens, final_temperature
        ));
        
        // æ„å»ºè¯·æ±‚
        let request = CreateChatCompletionRequestArgs::default()
            .model(model_name)
            .messages([
                ChatCompletionRequestMessage::System(
                    ChatCompletionRequestSystemMessage {
                        content: "You are a helpful assistant. Please respond briefly and concisely.".to_string(),
                        role: Role::System,
                        name: None,
                    },
                ),
                ChatCompletionRequestMessage::User(
                    async_openai::types::ChatCompletionRequestUserMessage {
                        role: async_openai::types::Role::User,
                        content: async_openai::types::ChatCompletionRequestUserMessageContent::Text(prompt.to_string()),
                        name: None,
                    },
                ),
            ])
            .max_tokens(final_max_tokens as u16)
            .temperature(final_temperature)
            .stream(false) // ä¸ä½¿ç”¨æµå¼è¾“å‡ºï¼Œç›´æ¥è·å–å®Œæ•´å“åº”
            .build()?;
        
        logger.info("AI_SERVICE", "ğŸ“¤ Sending chat completion request...");
        
        // å‘é€è¯·æ±‚
        let response = self.client.chat().create(request).await
            .map_err(|e| {
                logger.info("AI_SERVICE", &format!("âŒ Chat completion failed: {}", e));
                format!("Chat completion failed: {}", e)
            })?;
        
        // æå–å“åº”å†…å®¹
        if let Some(choice) = response.choices.first() {
            if let Some(content) = &choice.message.content {
                logger.info("AI_SERVICE", &format!(
                    "âœ… Chat completion successful - Response length: {} chars",
                    content.len()
                ));
                return Ok(content.clone());
            }
        }
        
        Err("No response content received".into())
    }

    /// è§£æ CSV æ ¼å¼çš„å•è¯æå–å“åº”
    fn parse_csv_response(
        &self,
        content: &str,
        logger: &Logger,
    ) -> Result<Vec<crate::types::word_analysis::ExtractedWord>, Box<dyn std::error::Error>> {
        // æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ¼å¼
        let cleaned_content = self.clean_csv_markdown(content);
        
        let mut words = Vec::new();
        let lines: Vec<&str> = cleaned_content.lines().collect();
        
        // è·³è¿‡æ ‡é¢˜è¡Œï¼ˆç¬¬ä¸€è¡Œï¼‰
        for line in lines.iter().skip(1) {
            let trimmed = line.trim();
            if trimmed.is_empty() {
                continue;
            }
            
            // è§£æ CSV è¡Œï¼šæ ¼å¼ä¸º "word,frequency"
            let parts: Vec<&str> = trimmed.split(',').collect();
            if parts.len() >= 2 {
                let word = parts[0].trim().to_string();
                let frequency = parts[1].trim().parse::<i32>().unwrap_or(1);
                
                if !word.is_empty() && word.len() >= 2 && word.len() <= 20 {
                    words.push(crate::types::word_analysis::ExtractedWord {
                        word,
                        frequency,
                    });
                }
            }
        }
        
        if words.is_empty() {
            logger.info("AI_SERVICE", &format!("âŒ CSV parsing failed: no valid words found"));
            logger.info("AI_SERVICE", &format!("ğŸ“„ Response content: {}", content));
            return Err("No valid words found in CSV response".into());
        }
        
        Ok(words)
    }

    /// æ¸…ç†CSVå“åº”ä¸­çš„markdownä»£ç å—æ ¼å¼
    fn clean_csv_markdown(&self, content: &str) -> String {
        let cleaned = content.trim();
        
        // ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if cleaned.starts_with("```csv") {
            cleaned[6..].trim_end_matches('`').to_string()
        } else if cleaned.starts_with("```") {
            // ç§»é™¤ä»»ä½•ä»£ç å—æ ‡è®°
            let lines: Vec<&str> = cleaned.lines().collect();
            if lines.len() > 1 {
                lines[1..lines.len()-1].join("\n")
            } else {
                cleaned.to_string()
            }
        } else {
            cleaned.to_string()
        }
    }

    /// è§£ææ‰¹é‡åˆ†æçš„ CSV å“åº”
    fn parse_batch_csv_response(
        &self,
        content: &str,
        logger: &Logger,
    ) -> Result<Vec<PhonicsWord>, Box<dyn std::error::Error>> {
        // æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ¼å¼
        let cleaned_content = self.clean_csv_markdown(content);
        
        let mut words = Vec::new();
        let mut rdr = csv::ReaderBuilder::new()
            .has_headers(true)
            .from_reader(cleaned_content.as_bytes());
        
        for result in rdr.deserialize::<CsvPhonicsRecord>() {
            match result {
                Ok(record) => {
                    words.push(record.into_phonics_word());
                }
                Err(e) => {
                    logger.info("AI_SERVICE", &format!("âŒ CSV parsing error for row: {}", e));
                    // ç»§ç»­å¤„ç†å…¶ä»–è¡Œï¼Œä¸ä¸­æ–­æ•´ä¸ªæ‰¹æ¬¡
                }
            }
        }
        
        if words.is_empty() {
            logger.info("AI_SERVICE", &format!("âŒ CSV parsing failed: no valid words found"));
            logger.info("AI_SERVICE", &format!("ğŸ“„ Response content: {}", content));
            return Err("No valid words found in CSV response".into());
        }
        
        Ok(words)
    }


    /// ä»å“åº”ä¸­æå–JSONéƒ¨åˆ†
    fn extract_json_from_response(&self, content: &str) -> String {
        // æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
        if let Some(start) = content.find('{') {
            if let Some(end) = content.rfind('}') {
                if end > start {
                    return content[start..=end].to_string();
                }
            }
        }
        
        // å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„JSONï¼Œè¿”å›åŸå†…å®¹
        content.to_string()
    }
}

/// JSONå“åº”æ ¼å¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JsonPhonicsResponse {
    pub words: Vec<JsonPhonicsWord>,
}

/// JSONæ ¼å¼çš„å•è¯æ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JsonPhonicsWord {
    pub word: String,
    pub frequency: i32,
    pub chinese_translation: String,
    pub pos_abbreviation: String,
    pub pos_english: String,
    pub pos_chinese: String,
    pub ipa: String,
    pub syllables: String,
    pub phonics_rule: String,
    pub analysis_explanation: String,
}

/// è‡ªç„¶æ‹¼è¯»åˆ†æç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhonicsAnalysisResult {
    pub words: Vec<PhonicsWord>,
}

/// å•è¯çš„è‡ªç„¶æ‹¼è¯»åˆ†æ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhonicsWord {
    pub word: String,
    pub frequency: i32,
    pub chinese_translation: String,
    pub pos_abbreviation: String,
    pub pos_english: String,
    pub pos_chinese: String,
    pub ipa: String,
    pub syllables: String,
    pub phonics_rule: String,
    pub analysis_explanation: String,
}

impl From<JsonPhonicsWord> for PhonicsWord {
    fn from(json_word: JsonPhonicsWord) -> Self {
        Self {
            word: json_word.word,
            frequency: json_word.frequency,
            chinese_translation: json_word.chinese_translation,
            pos_abbreviation: json_word.pos_abbreviation,
            pos_english: json_word.pos_english,
            pos_chinese: json_word.pos_chinese,
            ipa: json_word.ipa,
            syllables: json_word.syllables,
            phonics_rule: json_word.phonics_rule,
            analysis_explanation: json_word.analysis_explanation,
        }
    }
}

impl Default for PhonicsWord {
    fn default() -> Self {
        Self {
            word: String::new(),
            frequency: 0,
            chinese_translation: String::new(),
            pos_abbreviation: String::new(),
            pos_english: String::new(),
            pos_chinese: String::new(),
            ipa: String::new(),
            syllables: String::new(),
            phonics_rule: String::new(),
            analysis_explanation: String::new(),
        }
    }
}
