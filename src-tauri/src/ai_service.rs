use crate::logger::Logger;
use crate::types::AIModelConfig;
use async_openai::{
    config::OpenAIConfig,
    types::{
        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessage,
        CreateChatCompletionRequestArgs, Role,
    },
    Client,
};
use futures_util::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::Instant;

/// åˆ†æè¿›åº¦çŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisProgress {
    pub status: String,           // "analyzing", "completed", "error"
    pub current_step: String,     // å½“å‰æ­¥éª¤æè¿°
    pub chunks_received: u32,     // å·²æ¥æ”¶çš„chunkæ•°é‡
    pub total_chars: usize,       // å·²æ¥æ”¶çš„æ€»å­—ç¬¦æ•°
    pub elapsed_seconds: f64,     // å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰
    pub error_message: Option<String>, // é”™è¯¯ä¿¡æ¯
}

/// å…¨å±€è¿›åº¦ç®¡ç†å™¨
pub struct ProgressManager {
    progress: Arc<Mutex<Option<AnalysisProgress>>>,
    cancelled: Arc<Mutex<bool>>,
}

// å…¨å±€è¿›åº¦ç®¡ç†å™¨å®ä¾‹
static GLOBAL_PROGRESS_MANAGER: std::sync::OnceLock<ProgressManager> = std::sync::OnceLock::new();

impl ProgressManager {
    pub fn new() -> Self {
        Self {
            progress: Arc::new(Mutex::new(None)),
            cancelled: Arc::new(Mutex::new(false)),
        }
    }

    pub fn start_analysis(&self) {
        // é‡ç½®å–æ¶ˆæ ‡å¿—
        let mut cancelled = self.cancelled.lock().unwrap();
        *cancelled = false;
        drop(cancelled);

        let mut progress = self.progress.lock().unwrap();
        *progress = Some(AnalysisProgress {
            status: "analyzing".to_string(),
            current_step: "å‡†å¤‡åˆ†æ...".to_string(),
            chunks_received: 0,
            total_chars: 0,
            elapsed_seconds: 0.0,
            error_message: None,
        });
    }

    pub fn update_step(&self, step: &str, start_time: Instant) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.current_step = step.to_string();
                p.elapsed_seconds = start_time.elapsed().as_secs_f64();
            }
        }
    }

    pub fn update_chunk(&self, chunks: u32, total_chars: usize, start_time: Instant) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.chunks_received = chunks;
                p.total_chars = total_chars;
                p.elapsed_seconds = start_time.elapsed().as_secs_f64();
            }
        }
    }

    pub fn complete_analysis(&self) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "completed".to_string();
                p.current_step = "åˆ†æå®Œæˆ".to_string();
            }
        }
    }

    pub fn error_analysis(&self, error: &str) {
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "error".to_string();
                p.current_step = "åˆ†æå¤±è´¥".to_string();
                p.error_message = Some(error.to_string());
            }
        }
    }

    pub fn get_progress(&self) -> Option<AnalysisProgress> {
        self.progress.lock().ok()?.clone()
    }

    pub fn clear_progress(&self) {
        let mut progress = self.progress.lock().unwrap();
        *progress = None;
    }

    pub fn cancel_analysis(&self) {
        let mut cancelled = self.cancelled.lock().unwrap();
        *cancelled = true;
        drop(cancelled);

        // æ›´æ–°è¿›åº¦çŠ¶æ€ä¸ºå·²å–æ¶ˆ
        if let Ok(mut progress) = self.progress.lock() {
            if let Some(ref mut p) = *progress {
                p.status = "cancelled".to_string();
                p.current_step = "åˆ†æå·²å–æ¶ˆ".to_string();
            }
        }
    }

    pub fn is_cancelled(&self) -> bool {
        self.cancelled.lock().map(|guard| *guard).unwrap_or(false)
    }
}

/// è·å–å…¨å±€è¿›åº¦ç®¡ç†å™¨
pub fn get_global_progress_manager() -> &'static ProgressManager {
    GLOBAL_PROGRESS_MANAGER.get_or_init(|| ProgressManager::new())
}

/// AI æœåŠ¡æä¾›å•†é…ç½®ï¼ˆåŠ¨æ€é…ç½®ï¼‰
#[derive(Debug, Clone)]
pub struct AIProvider {
    pub name: String,
    pub base_url: String,
    pub api_key: String,
    pub default_model: String,
}

impl AIProvider {
    pub fn get_api_key(&self) -> &str {
        &self.api_key
    }

    pub fn get_base_url(&self) -> &str {
        &self.base_url
    }

    pub fn get_default_model(&self) -> &str {
        &self.default_model
    }
}

// ç§»é™¤äº†ä¼ ç»Ÿè¯æ±‡åˆ†æç›¸å…³çš„ç»“æ„ä½“ï¼Œåªä¿ç•™è‡ªç„¶æ‹¼è¯»åˆ†æ

/// ç®€å•çš„èŠå¤©æ¶ˆæ¯
#[derive(Debug, Serialize, Deserialize)]
pub struct SimpleChatMessage {
    pub role: String,
    pub content: String,
}

/// èŠå¤©å®Œæˆè¯·æ±‚
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionRequest {
    pub model: String,
    pub messages: Vec<SimpleChatMessage>,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f32>,
}

/// èŠå¤©å®Œæˆå“åº”
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionChoice {
    pub message: SimpleChatMessage,
    pub finish_reason: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionResponse {
    pub choices: Vec<ChatCompletionChoice>,
    pub usage: Option<HashMap<String, u32>>,
}

/// é€šç”¨ AI æœåŠ¡
pub struct AIService {
    provider: AIProvider,
    client: Client<OpenAIConfig>,
}

impl AIService {
    /// ä½¿ç”¨æŒ‡å®šæä¾›å•†åˆ›å»º AI æœåŠ¡
    pub fn new(provider: AIProvider) -> Self {
        let config = OpenAIConfig::new()
            .with_api_key(&provider.api_key)
            .with_api_base(&provider.base_url);

        Self {
            provider,
            client: Client::with_config(config),
        }
    }

    /// ä»æ•°æ®åº“æ¨¡å‹é…ç½®åˆ›å»º AI æœåŠ¡
    pub fn from_model_config(
        model_config: &AIModelConfig,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let provider = AIProvider {
            name: model_config.provider.name.clone(),
            base_url: model_config.provider.base_url.clone(),
            api_key: model_config.provider.api_key.clone(),
            default_model: model_config.model_id.clone(),
        };

        Ok(Self::new(provider))
    }

    /// è·å–åˆ†æè¿›åº¦
    pub fn get_analysis_progress(&self) -> Option<AnalysisProgress> {
        get_global_progress_manager().get_progress()
    }

    /// æ¸…é™¤åˆ†æè¿›åº¦
    pub fn clear_analysis_progress(&self) {
        get_global_progress_manager().clear_progress()
    }

    // ç§»é™¤äº†ä¼ ç»Ÿè¯æ±‡åˆ†ææ–¹æ³•ï¼Œåªä¿ç•™è‡ªç„¶æ‹¼è¯»åˆ†æ

    /// è‡ªç„¶æ‹¼è¯»åˆ†æ
    pub async fn analyze_phonics(
        &self,
        text: &str,
        model_name: Option<&str>,
        max_tokens: Option<u32>,
        temperature: Option<f32>,
        extraction_mode: &str,
        logger: &Logger,
    ) -> Result<PhonicsAnalysisResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();

        // å¯åŠ¨è¿›åº¦è·Ÿè¸ª
        let progress_manager = get_global_progress_manager();
        progress_manager.start_analysis();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting phonics analysis for text: {}",
                if text.len() > 100 { &text[..100] } else { text }
            ),
        );

        // æ­¥éª¤1: è¯»å–æç¤ºè¯æ¨¡æ¿
        progress_manager.update_step("è¯»å–æç¤ºè¯æ¨¡æ¿...", start_time);
        let step1_start = std::time::Instant::now();
        let prompt_template = include_str!("prompts/phonics_agent.md");
        let step1_duration = step1_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Step 1 - Loaded prompt template: {} chars in {:?}",
                prompt_template.len(),
                step1_duration
            ),
        );

        // æ­¥éª¤2: æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆæ›¿æ¢å ä½ç¬¦ï¼‰
        progress_manager.update_step("æ„å»ºåˆ†ææç¤ºè¯...", start_time);
        let step2_start = std::time::Instant::now();

        // æ ¹æ®æå–æ¨¡å¼ç”Ÿæˆé¢å¤–çš„é¢„å¤„ç†æ­¥éª¤
        let additional_steps = if extraction_mode == "focus" {
            r#"4. è¿‡æ»¤ç®€å•è¯æ±‡ï¼šæ’é™¤ä»¥ä¸‹ç±»å‹çš„ç®€å•è¯æ±‡ï¼Œè¿™äº›è¯æ±‡ä¸é€‚åˆä½œä¸ºå­¦ä¹ é‡ç‚¹
   - å† è¯ï¼ša, an, the
   - åŸºç¡€ä»£è¯ï¼šI, you, he, she, it, we, they, me, him, her, us, them
   - åŸºç¡€beåŠ¨è¯ï¼šam, is, are, was, were, be, been, being
   - åŸºç¡€åŠ©åŠ¨è¯ï¼šdo, does, did, have, has, had, will, would, can, could, should, shall, may, might, must
   - åŸºç¡€ä»‹è¯ï¼šin, on, at, to, for, of, with, by, from, up, out, off, over, under
   - åŸºç¡€è¿è¯ï¼šand, or, but, so, if, when, then, than, as
   - åŸºç¡€å‰¯è¯ï¼šnot, no, yes, very, too, also, only, just, now, here, there
   - å•å­—æ¯è¯ï¼ša, I
   - è¿‡äºç®€å•çš„æ•°å­—è¯ï¼šone, two, three, four, five, six, seven, eight, nine, ten

5. é‡ç‚¹å…³æ³¨ï¼šä¼˜å…ˆåˆ†æå…·æœ‰å­¦ä¹ ä»·å€¼çš„å®è¯ï¼ˆåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯ï¼‰å’Œä¸­ç­‰éš¾åº¦ä»¥ä¸Šçš„åŠŸèƒ½è¯"#
        } else {
            // all æ¨¡å¼ä¸‹ä¸æ·»åŠ é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤
            ""
        };

        let system_message = prompt_template
            .replace("{original_text}", text)
            .replace("{additional_text_preprocessing_steps}", additional_steps);

        let step2_duration = step2_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“ Step 2 - Built complete prompt with {} mode: {} chars in {:?}",
                extraction_mode,
                system_message.len(),
                step2_duration
            ),
        );



        // æ­¥éª¤3: å‡†å¤‡æ¨¡å‹å‚æ•°
        progress_manager.update_step("å‡†å¤‡AIæ¨¡å‹å‚æ•°...", start_time);
        let step3_start = std::time::Instant::now();
        let actual_model_name = model_name.unwrap_or(self.provider.get_default_model());
        let final_max_tokens = max_tokens.unwrap_or(8000);
        let final_temperature = temperature.unwrap_or(0.1);
        let step3_duration = step3_start.elapsed();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”§ Step 3 - Model parameters prepared in {:?}:",
                step3_duration
            ),
        );
        logger.info("AI_SERVICE", &format!("   ğŸ“Š Model: {}", actual_model_name));
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ¯ Max tokens: {}", final_max_tokens),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸŒ¡ï¸  Temperature: {}", final_temperature),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸŒ Base URL: {}", self.provider.base_url),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key length: {}", self.provider.api_key.len()),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key preview: {}...",
                if self.provider.api_key.len() > 10 {
                    &self.provider.api_key[..10]
                } else {
                    &self.provider.api_key
                }
            ),
        );

        // æ­¥éª¤4: æ„å»º AI è¯·æ±‚ï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰
        progress_manager.update_step("æ„å»ºAIè¯·æ±‚...", start_time);
        let step4_start = std::time::Instant::now();
        let request = CreateChatCompletionRequestArgs::default()
            .model(actual_model_name)
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: system_message,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(final_max_tokens.min(65535) as u16)
            .temperature(final_temperature)
            .stream(true) // å¯ç”¨æµå¼è¾“å‡º
            .build()?;
        let step4_duration = step4_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”¨ Step 4 - Built AI streaming request in {:?}",
                step4_duration
            ),
        );

        // æ­¥éª¤5: å‘é€ AI API æµå¼è¯·æ±‚
        progress_manager.update_step("è¿æ¥AIæœåŠ¡...", start_time);
        let step5_start = std::time::Instant::now();
        logger.info(
            "AI_SERVICE",
            "ğŸŒ Step 5 - Sending streaming request to AI API...",
        );
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¤ Request details: {} tokens max, temp {}, to {}",
                final_max_tokens, final_temperature, self.provider.base_url
            ),
        );

        let mut stream = self.client.chat().create_stream(request).await
            .map_err(|e| {
                logger.info("AI_SERVICE", &format!("âŒ Stream creation failed: {}", e));
                format!("Stream creation failed: {}", e)
            })?;
        let step5_duration = step5_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¡ Step 5 - Stream connection established in {:?}",
                step5_duration
            ),
        );

        // æ­¥éª¤6: å¤„ç†æµå¼å“åº”
        let step6_start = std::time::Instant::now();
        logger.info(
            "AI_SERVICE",
            "âœ… Successfully established stream connection",
        );

        // æ­¥éª¤7: æ”¶é›†æµå¼å“åº”å†…å®¹
        progress_manager.update_step("æ¥æ”¶AIåˆ†æç»“æœ...", start_time);
        logger.info(
            "AI_SERVICE",
            "ğŸ“¡ Step 6 - Starting to collect streaming response...",
        );
        let mut full_content = String::new();
        let mut chunk_count = 0;
        let mut last_log_time = std::time::Instant::now();

        while let Some(result) = stream.next().await {
            // æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
            if progress_manager.is_cancelled() {
                logger.info("AI_SERVICE", "ğŸš« Analysis cancelled by user, stopping stream processing");
                // è¿”å›ä¸€ä¸ªç©ºçš„ç»“æœè€Œä¸æ˜¯é”™è¯¯
                return Ok(PhonicsAnalysisResult { words: vec![] });
            }

            match result {
                Ok(response) => {
                    chunk_count += 1;

                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta.content {
                            full_content.push_str(delta);

                            // æ›´æ–°è¿›åº¦ç®¡ç†å™¨
                            progress_manager.update_chunk(chunk_count, full_content.len(), start_time);

                            // æ¯5ç§’è®°å½•ä¸€æ¬¡è¿›åº¦
                            if last_log_time.elapsed().as_secs() >= 5 {
                                logger.info(
                                    "AI_SERVICE",
                                    &format!(
                                        "ğŸ“¥ Received {} chunks, total: {} chars",
                                        chunk_count,
                                        full_content.len()
                                    ),
                                );
                                last_log_time = std::time::Instant::now();
                            }
                        }
                    }
                }
                Err(e) => {
                    let total_duration = start_time.elapsed();
                    let error_msg = format!("Stream error: {}", e);
                    progress_manager.error_analysis(&error_msg);
                    logger.info(
                        "AI_SERVICE",
                        &format!("âŒ Stream error after {:?}: {}", total_duration, e),
                    );
                    return Err(error_msg.into());
                }
            }
        }

        let step6_duration = step6_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¦ Step 6 - Collected {} chunks, {} total chars in {:?}",
                chunk_count,
                full_content.len(),
                step6_duration
            ),
        );

        // æ­¥éª¤8: è§£æå®Œæ•´çš„æµå¼å“åº”å†…å®¹
        let _step8_start = std::time::Instant::now();
        if !full_content.is_empty() {
            let content_length = full_content.len();
            logger.info(
                "AI_SERVICE",
                &format!(
                    "ğŸ“„ Step 8 - Processing complete response: {} chars",
                    content_length
                ),
            );

            // æ˜¾ç¤ºå“åº”å†…å®¹çš„å‰500å­—ç¬¦ç”¨äºè°ƒè¯•ï¼ˆå®‰å…¨æˆªå–ï¼Œé¿å… UTF-8 è¾¹ç•Œé—®é¢˜ï¼‰
            let preview = if full_content.len() > 500 {
                full_content.chars().take(500).collect::<String>()
            } else {
                full_content.clone()
            };
            logger.info(
                "AI_SERVICE",
                &format!("ğŸ” AI Response Preview: {}", preview),
            );

            // æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å« JSON ç»“æ„
            let has_opening_brace = full_content.contains("{");
            let has_words_array = full_content.contains("\"words\"");
            let has_closing_brace = full_content.contains("}");

            logger.info("AI_SERVICE", &format!("ğŸ” JSON Structure Check:"));
            logger.info(
                "AI_SERVICE",
                &format!("   ğŸ“‹ Has opening brace: {}", has_opening_brace),
            );
            logger.info(
                "AI_SERVICE",
                &format!("   ğŸ“ Has words array: {}", has_words_array),
            );
            logger.info(
                "AI_SERVICE",
                &format!("   ğŸ“‹ Has closing brace: {}", has_closing_brace),
            );

            if has_opening_brace && has_closing_brace {
                logger.info("AI_SERVICE", "âœ… Response contains expected JSON structure");
            } else {
                logger.info(
                    "AI_SERVICE",
                    "âš ï¸  Response does not contain complete JSON structure",
                );
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "ğŸ“„ First 1000 chars: {}",
                        full_content.chars().take(1000).collect::<String>()
                    ),
                );
            }

            // æ­¥éª¤9: è§£æ JSON å“åº”
            let step9_start = std::time::Instant::now();
            match self.parse_phonics_json(&full_content) {
                Ok(result) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "ğŸ‰ Step 9 - Successfully parsed {} phonics entries in {:?}",
                            result.words.len(),
                            step9_duration
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total analysis time: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå®Œæˆ
                    progress_manager.complete_analysis();

                    Ok(result)
                }
                Err(e) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "âŒ Step 9 - XML parsing failed in {:?}: {}",
                            step9_duration, e
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("ğŸ“„ Full AI response for debugging: {}", full_content),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total time before failure: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå¤±è´¥
                    let error_msg = format!("Failed to parse phonics analysis: JSON parsing error: {}", e);
                    progress_manager.error_analysis(&error_msg);

                    Err(error_msg.into())
                }
            }
        } else {
            let total_duration = start_time.elapsed();
            logger.info(
                "AI_SERVICE",
                &format!(
                    "âŒ No content received from streaming response after {:?}",
                    total_duration
                ),
            );
            Err("No content received from streaming response".into())
        }
    }

    /// ç”Ÿæˆå­¦ä¹ è®¡åˆ’æ—¥ç¨‹è§„åˆ’
    pub async fn generate_study_plan_schedule(
        &self,
        params: crate::types::study::StudyPlanAIParams,
        model_config: &AIModelConfig,
        logger: &Logger,
    ) -> Result<crate::types::study::StudyPlanAIResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();

        // å¯åŠ¨è¿›åº¦è·Ÿè¸ª
        let progress_manager = get_global_progress_manager();
        progress_manager.start_analysis();

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸš€ Starting study plan schedule generation: {} words, {} days, {} intensity",
                params.total_words, params.period_days, params.intensity_level
            ),
        );

        // æ­¥éª¤1: è¯»å–æç¤ºè¯æ¨¡æ¿
        progress_manager.update_step("è¯»å–å­¦ä¹ è®¡åˆ’æç¤ºè¯æ¨¡æ¿...", start_time);
        let step1_start = std::time::Instant::now();
        let prompt_template = include_str!("prompts/study_plan_agent.md");
        let step1_duration = step1_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“„ Step 1 - Loaded study plan prompt template: {} chars in {:?}",
                prompt_template.len(),
                step1_duration
            ),
        );

        // æ­¥éª¤2: æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆæ›¿æ¢å ä½ç¬¦ï¼‰
        progress_manager.update_step("æ„å»ºå­¦ä¹ è®¡åˆ’è§„åˆ’æç¤ºè¯...", start_time);
        let step2_start = std::time::Instant::now();

        // å°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        let word_list_json = match serde_json::to_string_pretty(&params.word_list) {
            Ok(json) => json,
            Err(e) => {
                let error_msg = format!("Failed to serialize word list: {}", e);
                progress_manager.error_analysis(&error_msg);
                return Err(error_msg.into());
            }
        };

        // æ›¿æ¢æç¤ºè¯ä¸­çš„å ä½ç¬¦
        let full_prompt = prompt_template
            .replace("{intensity_level}", &params.intensity_level)
            .replace("{total_words}", &params.total_words.to_string())
            .replace("{period_days}", &params.period_days.to_string())
            .replace("{review_frequency}", &params.review_frequency.to_string())
            .replace("{start_date}", &params.start_date)
            .replace("{word_list}", &word_list_json);

        let step2_duration = step2_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“ Step 2 - Built full prompt: {} chars in {:?}",
                full_prompt.len(),
                step2_duration
            ),
        );

        // è¾“å‡ºå®Œæ•´æç¤ºè¯ç”¨äºè°ƒè¯•
        logger.info(
            "AI_SERVICE",
            &format!("ğŸ“ Full prompt content:\n{}", full_prompt),
        );

        // æ­¥éª¤3: å‡†å¤‡APIè¯·æ±‚å‚æ•°
        progress_manager.update_step("å‡†å¤‡AIè¯·æ±‚å‚æ•°...", start_time);
        let step3_start = std::time::Instant::now();

        let model_name = model_config.model_id.as_str();
        let max_tokens = model_config.max_tokens.unwrap_or(4000) as u32;
        let temperature = model_config.temperature.unwrap_or(0.3) as f32;

        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ”§ Step 3 - API params: model={}, max_tokens={}, temperature={}",
                model_name, max_tokens, temperature
            ),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”— Base URL: {}", self.provider.base_url),
        );
        logger.info(
            "AI_SERVICE",
            &format!("   ğŸ”‘ API key preview: {}...",
                if self.provider.api_key.len() > 10 {
                    &self.provider.api_key[..10]
                } else {
                    &self.provider.api_key
                }
            ),
        );

        // æµ‹è¯•ç½‘ç»œè¿æ¥
        logger.info(
            "AI_SERVICE",
            "ğŸŒ Testing network connectivity to OpenRouter...",
        );

        let step3_duration = step3_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("âš™ï¸  Step 3 completed in {:?}", step3_duration),
        );

        // æ­¥éª¤4: åˆ›å»ºèŠå¤©è¯·æ±‚
        progress_manager.update_step("åˆ›å»ºAIèŠå¤©è¯·æ±‚...", start_time);
        let step4_start = std::time::Instant::now();

        let request = CreateChatCompletionRequestArgs::default()
            .model(model_name)
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: full_prompt,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(max_tokens.min(65535) as u16)
            .temperature(temperature)
            .stream(true)
            .build()?;

        let step4_duration = step4_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("ğŸ“¤ Step 4 - Created chat request in {:?}", step4_duration),
        );

        // æ­¥éª¤5: å‘é€è¯·æ±‚å¹¶å»ºç«‹æµè¿æ¥
        progress_manager.update_step("å‘é€AIè¯·æ±‚...", start_time);
        let step5_start = std::time::Instant::now();

        let stream = self.client.chat().create_stream(request).await;

        let step5_duration = step5_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!("ğŸŒ Step 5 - Sent request in {:?}", step5_duration),
        );

        // æ­¥éª¤6: å¤„ç†æµå¼å“åº”
        let step6_start = std::time::Instant::now();
        let mut stream = match stream {
            Ok(s) => {
                logger.info(
                    "AI_SERVICE",
                    "âœ… Successfully established stream connection for study plan",
                );
                s
            }
            Err(e) => {
                let total_duration = start_time.elapsed();
                let error_msg = format!("Failed to establish stream connection: {}", e);
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "âŒ Stream connection failed after {:?}: {}",
                        total_duration, error_msg
                    ),
                );
                progress_manager.error_analysis(&error_msg);
                return Err(error_msg.into());
            }
        };

        // æ­¥éª¤7: æ”¶é›†æµå¼å“åº”å†…å®¹
        progress_manager.update_step("æ¥æ”¶AIå­¦ä¹ è®¡åˆ’è§„åˆ’ç»“æœ...", start_time);
        logger.info(
            "AI_SERVICE",
            "ğŸ“¥ Step 7 - Starting to receive study plan stream response",
        );

        let mut full_content = String::new();
        let mut chunk_count = 0;
        let mut last_log_time = std::time::Instant::now();

        while let Some(result) = stream.next().await {
            // æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
            if progress_manager.is_cancelled() {
                logger.info("AI_SERVICE", "ğŸš« Study plan generation cancelled by user");
                // è¿”å›ä¸€ä¸ªç©ºçš„ç»“æœè€Œä¸æ˜¯é”™è¯¯ï¼Œä¸è‡ªç„¶æ‹¼è¯»åˆ†æä¿æŒä¸€è‡´
                return Ok(crate::types::study::StudyPlanAIResult {
                    plan_metadata: crate::types::study::StudyPlanMetadata {
                        total_words: 0,
                        study_period_days: 0,
                        intensity_level: "".to_string(),
                        review_frequency: 0,
                        plan_type: "".to_string(),
                        start_date: "".to_string(),
                        end_date: "".to_string(),
                    },
                    daily_plans: vec![],
                });
            }

            match result {
                Ok(response) => {
                    chunk_count += 1;

                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta.content {
                            full_content.push_str(delta);

                            // æ›´æ–°è¿›åº¦ç®¡ç†å™¨
                            progress_manager.update_chunk(chunk_count, full_content.len(), start_time);

                            // æ¯5ç§’è®°å½•ä¸€æ¬¡è¿›åº¦
                            if last_log_time.elapsed().as_secs() >= 5 {
                                logger.info(
                                    "AI_SERVICE",
                                    &format!(
                                        "ğŸ“¥ Received {} chunks, total: {} chars",
                                        chunk_count,
                                        full_content.len()
                                    ),
                                );
                                last_log_time = std::time::Instant::now();
                            }
                        }
                    }
                }
                Err(e) => {
                    let total_duration = start_time.elapsed();
                    let error_msg = format!("Stream error: {}", e);
                    progress_manager.error_analysis(&error_msg);

                    // è¯¦ç»†çš„ç½‘ç»œé”™è¯¯è¯Šæ–­
                    logger.info(
                        "AI_SERVICE",
                        &format!("âŒ Stream error after {:?}: {}", total_duration, e),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("ğŸ” Network Error Details:"),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ“¡ Target URL: {}", self.provider.base_url),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ”‘ API Key Length: {} chars", self.provider.api_key.len()),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   â±ï¸  Request Duration: {:?}", total_duration),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("   ğŸ¯ Model: {}", model_name),
                    );

                    // æ£€æŸ¥é”™è¯¯ç±»å‹
                    let error_str = e.to_string();
                    if error_str.contains("dns") || error_str.contains("resolve") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Possible DNS resolution issue. Check internet connection.",
                        );
                    } else if error_str.contains("timeout") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Request timeout. The API might be slow or overloaded.",
                        );
                    } else if error_str.contains("connection") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Connection issue. Check firewall or proxy settings.",
                        );
                    } else if error_str.contains("401") || error_str.contains("unauthorized") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Authentication issue. Check API key validity.",
                        );
                    } else if error_str.contains("429") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Rate limit exceeded. Wait before retrying.",
                        );
                    } else if error_str.contains("500") || error_str.contains("502") || error_str.contains("503") {
                        logger.info(
                            "AI_SERVICE",
                            "ğŸ’¡ Server error. The API service might be temporarily unavailable.",
                        );
                    }

                    return Err(error_msg.into());
                }
            }
        }

        let step6_duration = step6_start.elapsed();
        logger.info(
            "AI_SERVICE",
            &format!(
                "ğŸ“¥ Step 6 - Received {} chunks, {} chars in {:?}",
                chunk_count,
                full_content.len(),
                step6_duration
            ),
        );

        // æ­¥éª¤8: éªŒè¯å“åº”å†…å®¹
        if !full_content.is_empty() {
            logger.info(
                "AI_SERVICE",
                &format!(
                    "ğŸ“„ Step 8 - Response content length: {} chars",
                    full_content.len()
                ),
            );

            // è¾“å‡ºAIçš„åŸå§‹è¿”å›å†…å®¹ç”¨äºè°ƒè¯•
            logger.info(
                "AI_SERVICE",
                &format!("ğŸ“„ AIåŸå§‹è¿”å›å†…å®¹:\n{}", full_content),
            );

            // æ£€æŸ¥æ˜¯å¦åŒ…å«JSONç»“æ„
            if !full_content.contains("plan_metadata") || !full_content.contains("daily_plans") {
                logger.info(
                    "AI_SERVICE",
                    "âš ï¸  Response does not contain complete study plan JSON structure",
                );
                logger.info(
                    "AI_SERVICE",
                    &format!(
                        "ğŸ“„ First 1000 chars: {}",
                        full_content.chars().take(1000).collect::<String>()
                    ),
                );
            }

            // æ­¥éª¤9: è§£æ JSON å“åº”
            let step9_start = std::time::Instant::now();
            match self.parse_study_plan_json(&full_content) {
                Ok(result) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "âœ… Step 9 - Successfully parsed study plan with {} daily plans in {:?}",
                            result.daily_plans.len(), step9_duration
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total study plan generation time: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå®Œæˆ
                    progress_manager.complete_analysis();

                    Ok(result)
                }
                Err(e) => {
                    let step9_duration = step9_start.elapsed();
                    let total_duration = start_time.elapsed();
                    logger.info(
                        "AI_SERVICE",
                        &format!(
                            "âŒ Step 9 - Study plan JSON parsing failed in {:?}: {}",
                            step9_duration, e
                        ),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("ğŸ“„ Full AI response for debugging: {}", full_content),
                    );
                    logger.info(
                        "AI_SERVICE",
                        &format!("â±ï¸  Total time before failure: {:?}", total_duration),
                    );

                    // æ ‡è®°åˆ†æå¤±è´¥
                    let error_msg = format!("Failed to parse study plan: JSON parsing error: {}", e);
                    progress_manager.error_analysis(&error_msg);

                    Err(error_msg.into())
                }
            }
        } else {
            let total_duration = start_time.elapsed();
            logger.info(
                "AI_SERVICE",
                &format!(
                    "âŒ No content received from study plan streaming response after {:?}",
                    total_duration
                ),
            );
            let error_msg = "No content received from streaming response";
            progress_manager.error_analysis(error_msg);
            Err(error_msg.into())
        }
    }

    /// è§£æå­¦ä¹ è®¡åˆ’è§„åˆ’çš„ JSON å“åº”
    fn parse_study_plan_json(
        &self,
        json_content: &str,
    ) -> Result<crate::types::study::StudyPlanAIResult, Box<dyn std::error::Error>> {
        // æå–JSONéƒ¨åˆ†
        let json_str = self.extract_json_from_response(json_content);

        // å…ˆè§£æä¸ºé€šç”¨çš„JSONå€¼ï¼Œç„¶åè¿›è¡Œå­—æ®µåè½¬æ¢
        let ai_json: serde_json::Value = serde_json::from_str(&json_str)?;

        // è½¬æ¢å­—æ®µåä»ä¸‹åˆ’çº¿åˆ°é©¼å³°å‘½å
        let converted_json = self.convert_study_plan_field_names(ai_json)?;

        // è§£æä¸ºæœ€ç»ˆçš„ç»“æ„ä½“
        let result: crate::types::study::StudyPlanAIResult = serde_json::from_value(converted_json)?;

        // éªŒè¯ç»“æœ
        if result.daily_plans.is_empty() {
            return Err("No daily plans found in JSON response".into());
        }

        Ok(result)
    }

    /// è½¬æ¢å­¦ä¹ è®¡åˆ’JSONçš„å­—æ®µåä»ä¸‹åˆ’çº¿åˆ°é©¼å³°å‘½å
    fn convert_study_plan_field_names(
        &self,
        mut json: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = json {
            // è½¬æ¢é¡¶çº§å­—æ®µ
            if let Some(plan_metadata) = obj.remove("plan_metadata") {
                obj.insert("planMetadata".to_string(), self.convert_metadata_fields(plan_metadata)?);
            }
            if let Some(daily_plans) = obj.remove("daily_plans") {
                obj.insert("dailyPlans".to_string(), self.convert_daily_plans_fields(daily_plans)?);
            }
        }
        Ok(json)
    }

    /// è½¬æ¢å…ƒæ•°æ®å­—æ®µå
    fn convert_metadata_fields(
        &self,
        mut metadata: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = metadata {
            // è½¬æ¢å­—æ®µå
            if let Some(val) = obj.remove("total_words") {
                obj.insert("totalWords".to_string(), val);
            }
            if let Some(val) = obj.remove("study_period_days") {
                obj.insert("studyPeriodDays".to_string(), val);
            }
            if let Some(val) = obj.remove("intensity_level") {
                obj.insert("intensityLevel".to_string(), val);
            }
            if let Some(val) = obj.remove("review_frequency") {
                obj.insert("reviewFrequency".to_string(), val);
            }
            if let Some(val) = obj.remove("plan_type") {
                obj.insert("planType".to_string(), val);
            }
            if let Some(val) = obj.remove("start_date") {
                obj.insert("startDate".to_string(), val);
            }
            if let Some(val) = obj.remove("end_date") {
                obj.insert("endDate".to_string(), val);
            }
        }
        Ok(metadata)
    }

    /// è½¬æ¢æ¯æ—¥è®¡åˆ’å­—æ®µå
    fn convert_daily_plans_fields(
        &self,
        daily_plans: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Array(plans) = daily_plans {
            let converted_plans: Result<Vec<serde_json::Value>, _> = plans
                .into_iter()
                .map(|plan| self.convert_daily_plan_fields(plan))
                .collect();
            Ok(serde_json::Value::Array(converted_plans?))
        } else {
            Ok(daily_plans)
        }
    }

    /// è½¬æ¢å•ä¸ªæ¯æ—¥è®¡åˆ’çš„å­—æ®µå
    fn convert_daily_plan_fields(
        &self,
        mut plan: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = plan {
            // è½¬æ¢wordsæ•°ç»„
            if let Some(words) = obj.remove("words") {
                if let serde_json::Value::Array(word_list) = words {
                    let converted_words: Result<Vec<serde_json::Value>, _> = word_list
                        .into_iter()
                        .map(|word| self.convert_word_fields(word))
                        .collect();
                    obj.insert("words".to_string(), serde_json::Value::Array(converted_words?));
                }
            }
        }
        Ok(plan)
    }

    /// è½¬æ¢å•è¯å­—æ®µå
    fn convert_word_fields(
        &self,
        mut word: serde_json::Value,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        if let serde_json::Value::Object(ref mut obj) = word {
            // è½¬æ¢å­—æ®µå
            if let Some(val) = obj.remove("word_id") {
                obj.insert("wordId".to_string(), val);
            }
            if let Some(val) = obj.remove("wordbook_id") {
                obj.insert("wordbookId".to_string(), val);
            }
            if let Some(val) = obj.remove("is_review") {
                obj.insert("isReview".to_string(), val);
            }
            if let Some(val) = obj.remove("review_count") {
                obj.insert("reviewCount".to_string(), val);
            }
            if let Some(val) = obj.remove("difficulty_level") {
                obj.insert("difficultyLevel".to_string(), val);
            }
        }
        Ok(word)
    }

    /// è§£æè‡ªç„¶æ‹¼è¯»åˆ†æçš„ JSON å“åº”
    fn parse_phonics_json(
        &self,
        json_content: &str,
    ) -> Result<PhonicsAnalysisResult, Box<dyn std::error::Error>> {
        // æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤å¯èƒ½çš„å‰åæ–‡æœ¬ï¼‰
        let json_str = self.extract_json_from_response(json_content);

        // è§£æJSON
        let json_response: JsonPhonicsResponse = serde_json::from_str(&json_str)
            .map_err(|e| format!("JSON parsing error: {}", e))?;

        // è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼
        let words: Vec<PhonicsWord> = json_response.words
            .into_iter()
            .filter(|w| !w.word.is_empty()) // è¿‡æ»¤ç©ºå•è¯
            .map(|w| w.into())
            .collect();

        if words.is_empty() {
            return Err("No valid words found in JSON response".into());
        }

        Ok(PhonicsAnalysisResult { words })
    }

    /// ä»å“åº”ä¸­æå–JSONéƒ¨åˆ†
    fn extract_json_from_response(&self, content: &str) -> String {
        // æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
        if let Some(start) = content.find('{') {
            if let Some(end) = content.rfind('}') {
                if end > start {
                    return content[start..=end].to_string();
                }
            }
        }

        // å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„JSONï¼Œè¿”å›åŸå†…å®¹
        content.to_string()
    }
}

/// JSONå“åº”æ ¼å¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JsonPhonicsResponse {
    pub words: Vec<JsonPhonicsWord>,
}

/// JSONæ ¼å¼çš„å•è¯æ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JsonPhonicsWord {
    pub word: String,
    pub frequency: i32,
    pub chinese_translation: String,
    pub pos_abbreviation: String,
    pub pos_english: String,
    pub pos_chinese: String,
    pub ipa: String,
    pub syllables: String,
    pub phonics_rule: String,
    pub analysis_explanation: String,
}

/// è‡ªç„¶æ‹¼è¯»åˆ†æç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhonicsAnalysisResult {
    pub words: Vec<PhonicsWord>,
}

/// å•è¯çš„è‡ªç„¶æ‹¼è¯»åˆ†æ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhonicsWord {
    pub word: String,
    pub frequency: i32,
    pub chinese_translation: String,
    pub pos_abbreviation: String,
    pub pos_english: String,
    pub pos_chinese: String,
    pub ipa: String,
    pub syllables: String,
    pub phonics_rule: String,
    pub analysis_explanation: String,
}

impl From<JsonPhonicsWord> for PhonicsWord {
    fn from(json_word: JsonPhonicsWord) -> Self {
        Self {
            word: json_word.word,
            frequency: json_word.frequency,
            chinese_translation: json_word.chinese_translation,
            pos_abbreviation: json_word.pos_abbreviation,
            pos_english: json_word.pos_english,
            pos_chinese: json_word.pos_chinese,
            ipa: json_word.ipa,
            syllables: json_word.syllables,
            phonics_rule: json_word.phonics_rule,
            analysis_explanation: json_word.analysis_explanation,
        }
    }
}

impl Default for PhonicsWord {
    fn default() -> Self {
        Self {
            word: String::new(),
            frequency: 0,
            chinese_translation: String::new(),
            pos_abbreviation: String::new(),
            pos_english: String::new(),
            pos_chinese: String::new(),
            ipa: String::new(),
            syllables: String::new(),
            phonics_rule: String::new(),
            analysis_explanation: String::new(),
        }
    }
}
