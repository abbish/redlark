# æ‰¹é‡åˆ†æè¿›åº¦æ›´æ–°é—®é¢˜åˆ†æä¸æ–¹æ¡ˆè®¾è®¡

## ä¸€ã€å½“å‰é—®é¢˜åˆ†æ

### 1.1 è½®è¯¢æœºåˆ¶çš„æ ¹æœ¬é—®é¢˜

**å½“å‰å®ç°**ï¼š
```typescript
// å‰ç«¯æ¯ 500ms è½®è¯¢ä¸€æ¬¡
const pollInterval = setInterval(async () => {
  const progress = await wordAnalysisService.getBatchAnalysisProgress();
  // æ›´æ–° UI
}, 500);
```

**é—®é¢˜è¯Šæ–­**ï¼š

1. **æ—¶åºé—®é¢˜**ï¼šåç«¯åœ¨å¼‚æ­¥é—­åŒ…ä¸­æ›´æ–°è¿›åº¦ï¼Œä½†å‰ç«¯è½®è¯¢å¯èƒ½é”™è¿‡ä¸­é—´çŠ¶æ€
2. **ç«äº‰æ¡ä»¶**ï¼šå¤šä¸ªæ‰¹æ¬¡å¹¶è¡Œæ‰§è¡Œæ—¶ï¼Œè¿›åº¦æ›´æ–°å¯èƒ½è¢«è¦†ç›–æˆ–ä¸¢å¤±
3. **å»¶è¿Ÿç´¯ç§¯**ï¼šè½®è¯¢é—´éš” + ç½‘ç»œå»¶è¿Ÿ + åç«¯å¤„ç†å»¶è¿Ÿï¼Œå¯¼è‡´ UI æ›´æ–°æ»å
4. **æ— æ•ˆè¯·æ±‚**ï¼šåœ¨æ‰¹æ¬¡å¤„ç†æœŸé—´ï¼Œè½®è¯¢å¯èƒ½è¿”å›ç›¸åŒçš„è¿›åº¦ï¼Œæµªè´¹èµ„æº

### 1.2 å¹¶å‘æ‰¹æ¬¡çš„è¿›åº¦ç®¡ç†é—®é¢˜

**å½“å‰å®ç°**ï¼š
```rust
// åœ¨ async move é—­åŒ…ä¸­æ›´æ–°è¿›åº¦
async move {
    progress_manager.update_word_status(/* ... */);
    progress_manager.update_analysis_progress(/* ... */);
    // å¤„ç†æ‰¹æ¬¡
}
```

**é—®é¢˜**ï¼š
- 5 ä¸ªå¹¶å‘æ‰¹æ¬¡åŒæ—¶æ›´æ–° `EnhancedProgressManager`
- `Arc<Mutex>` çš„é”ç«äº‰å¯èƒ½å¯¼è‡´æ›´æ–°å»¶è¿Ÿ
- æ‰¹æ¬¡å®Œæˆé¡ºåºä¸ç¡®å®šï¼Œè¿›åº¦è®¡ç®—å¯èƒ½ä¸å‡†ç¡®

## äºŒã€äº‹ä»¶æ¨é€æ–¹æ¡ˆè®¾è®¡

### 2.1 æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph Frontend
        A[WordImporterModal] -->|ç›‘å¬äº‹ä»¶| B[Event Listener]
        B -->|æ›´æ–° UI| A
    end
    
    subgraph Backend
        C[analyze_words_parallel] -->|æ‰¹æ¬¡å¼€å§‹| D[emit: batch-start]
        C -->|å•è¯çŠ¶æ€æ›´æ–°| E[emit: word-status-update]
        C -->|æ‰¹æ¬¡å®Œæˆ| F[emit: batch-complete]
        C -->|åˆ†æå®Œæˆ| G[emit: analysis-complete]
        C -->|åˆ†æå¤±è´¥| H[emit: analysis-error]
    end
    
    D -->|Tauri Event| B
    E -->|Tauri Event| B
    F -->|Tauri Event| B
    G -->|Tauri Event| B
    H -->|Tauri Event| B
```

### 2.2 äº‹ä»¶å®šä¹‰

```rust
// äº‹ä»¶ç±»å‹å®šä¹‰
#[derive(Serialize, Deserialize, Clone)]
pub struct BatchStartEvent {
    pub batch_index: usize,
    pub total_batches: usize,
    pub words: Vec<String>,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct WordStatusUpdateEvent {
    pub word: String,
    pub status: String, // "analyzing", "completed", "failed"
    pub error: Option<String>,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct BatchCompleteEvent {
    pub batch_index: usize,
    pub completed_words: usize,
    pub failed_words: usize,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct AnalysisCompleteEvent {
    pub total_words: usize,
    pub completed_words: usize,
    pub failed_words: usize,
    pub elapsed_seconds: f64,
}
```

### 2.3 åç«¯å®ç°

```rust
// åœ¨ word_analysis_handlers.rs ä¸­
use tauri::Emitter;

async fn analyze_words_parallel(
    ai_service: Arc<AIService>,
    words: Vec<String>,
    app_handle: AppHandle,  // æ–°å¢å‚æ•°
    logger: &Logger,
    progress_manager: &EnhancedProgressManager,
    config: &BatchAnalysisConfig,
) -> Result<BatchAnalysisResult, Box<dyn std::error::Error>> {
    let start_time = std::time::Instant::now();
    
    // ... æ‰¹æ¬¡å¤„ç†é€»è¾‘ ...
    
    let batches_stream = stream::iter(batches.into_iter().enumerate())
        .map(|(batch_index, batch_words)| {
            let ai_service = Arc::clone(&ai_service);
            let logger = logger.clone();
            let batch_index_clone = batch_index;
            let batch_words_clone = batch_words.clone();
            let app_handle = app_handle.clone();  // å…‹éš† AppHandle
            
            async move {
                // å‘é€æ‰¹æ¬¡å¼€å§‹äº‹ä»¶
                let _ = app_handle.emit("batch-start", BatchStartEvent {
                    batch_index: batch_index_clone,
                    total_batches,
                    words: batch_words_clone.clone(),
                });
                
                // æ›´æ–°å•è¯çŠ¶æ€ä¸º analyzing
                for word in &batch_words_clone {
                    let _ = app_handle.emit("word-status-update", WordStatusUpdateEvent {
                        word: word.clone(),
                        status: "analyzing".to_string(),
                        error: None,
                    });
                }
                
                // å¤„ç†æ‰¹æ¬¡
                match ai_service.analyze_words_batch(
                    batch_words_clone.clone(),
                    batch_index_clone,
                    total_batches,
                    &logger,
                ).await {
                    Ok(batch_results) => {
                        // å‘é€å•è¯å®Œæˆäº‹ä»¶
                        for word in &batch_results {
                            let _ = app_handle.emit("word-status-update", WordStatusUpdateEvent {
                                word: word.word.clone(),
                                status: "completed".to_string(),
                                error: None,
                            });
                        }
                        
                        // å‘é€æ‰¹æ¬¡å®Œæˆäº‹ä»¶
                        let _ = app_handle.emit("batch-complete", BatchCompleteEvent {
                            batch_index: batch_index_clone,
                            completed_words: batch_results.len(),
                            failed_words: 0,
                        });
                        
                        (batch_index, Some(batch_results), Vec::new())
                    }
                    Err(e) => {
                        // å‘é€å•è¯å¤±è´¥äº‹ä»¶
                        for word in &batch_words_clone {
                            let _ = app_handle.emit("word-status-update", WordStatusUpdateEvent {
                                word: word.clone(),
                                status: "failed".to_string(),
                                error: Some(e.to_string()),
                            });
                        }
                        
                        (batch_index, None, batch_words_clone)
                    }
                }
            }
        })
        .buffer_unordered(config.max_concurrent_batches as usize);
    
    // ... å¤„ç†æ‰¹æ¬¡æµ ...
    
    // å‘é€åˆ†æå®Œæˆäº‹ä»¶
    let _ = app_handle.emit("analysis-complete", AnalysisCompleteEvent {
        total_words: total_words_count,
        completed_words: analysis_results.len(),
        failed_words: failed_words.len(),
        elapsed_seconds: start_time.elapsed().as_secs_f64(),
    });
    
    Ok(result)
}
```

### 2.4 å‰ç«¯å®ç°

```typescript
// åœ¨ WordImporterModal.tsx ä¸­
import { listen, UnlistenFn } from '@tauri-apps/api/event';

const WordImporterModal: React.FC<WordImporterModalProps> = ({
  isOpen,
  onClose,
  onSaveWords,
  saving
}) => {
  const [currentStep, setCurrentStep] = useState<Step>('input');
  const [extractedWordList, setExtractedWordList] = useState<string[]>([]);
  const [wordAnalysisStatuses, setWordAnalysisStatuses] = useState<Record<string, WordAnalysisStatus>>({});
  const [batchProgress, setBatchProgress] = useState<{
    totalWords: number;
    completedWords: number;
    currentBatch: number;
    totalBatches: number;
  } | null>(null);
  
  // äº‹ä»¶ç›‘å¬å™¨å¼•ç”¨
  const eventListenersRef = useRef<UnlistenFn[]>([]);
  
  // è®¾ç½®äº‹ä»¶ç›‘å¬
  useEffect(() => {
    if (currentStep === 'batch-analysis') {
      const listeners: UnlistenFn[] = [];
      
      // ç›‘å¬æ‰¹æ¬¡å¼€å§‹äº‹ä»¶
      listen('batch-start', (event: any) => {
        const data = event.payload as BatchStartEvent;
        console.log('Batch started:', data);
        setBatchProgress(prev => prev ? {
          ...prev,
          currentBatch: data.batch_index,
          totalBatches: data.total_batches,
        } : null);
      }).then(unlisten => listeners.push(unlisten));
      
      // ç›‘å¬å•è¯çŠ¶æ€æ›´æ–°äº‹ä»¶
      listen('word-status-update', (event: any) => {
        const data = event.payload as WordStatusUpdateEvent;
        console.log('Word status updated:', data);
        setWordAnalysisStatuses(prev => ({
          ...prev,
          [data.word]: data.status as WordAnalysisStatus,
        }));
      }).then(unlisten => listeners.push(unlisten));
      
      // ç›‘å¬æ‰¹æ¬¡å®Œæˆäº‹ä»¶
      listen('batch-complete', (event: any) => {
        const data = event.payload as BatchCompleteEvent;
        console.log('Batch completed:', data);
        setBatchProgress(prev => prev ? {
          ...prev,
          completedWords: prev.completedWords + data.completed_words,
        } : null);
      }).then(unlisten => listeners.push(unlisten));
      
      // ç›‘å¬åˆ†æå®Œæˆäº‹ä»¶
      listen('analysis-complete', (event: any) => {
        const data = event.payload as AnalysisCompleteEvent;
        console.log('Analysis completed:', data);
        // è½¬æ¢ç»“æœå¹¶è·³è½¬åˆ°ç»“æœæ­¥éª¤
        // ...
      }).then(unlisten => listeners.push(unlisten));
      
      // ç›‘å¬åˆ†æé”™è¯¯äº‹ä»¶
      listen('analysis-error', (event: any) => {
        const data = event.payload as { message: string };
        console.error('Analysis error:', data);
        setBatchError(data.message);
      }).then(unlisten => listeners.push(unlisten));
      
      // ä¿å­˜ç›‘å¬å™¨å¼•ç”¨
      eventListenersRef.current = listeners;
    }
    
    // æ¸…ç†å‡½æ•°
    return () => {
      eventListenersRef.current.forEach(unlisten => unlisten());
      eventListenersRef.current = [];
    };
  }, [currentStep]);
  
  // ... å…¶ä»–ä»£ç  ...
};
```

### 2.5 ä¼˜åŠ¿åˆ†æ

| æ–¹é¢ | è½®è¯¢æœºåˆ¶ | äº‹ä»¶æ¨é€ |
|------|-----------|---------|
| å®æ—¶æ€§ | 500ms å»¶è¿Ÿ | æ¯«ç§’çº§å»¶è¿Ÿ |
| ç½‘ç»œå¼€é”€ | æŒç»­è¯·æ±‚ | ä»…äº‹ä»¶è§¦å‘æ—¶ |
| æœåŠ¡å™¨è´Ÿè½½ | æŒç»­æŸ¥è¯¢ | æŒ‰éœ€æ¨é€ |
| ä»£ç å¤æ‚åº¦ | ç®€å• | ä¸­ç­‰ |
| å¯é æ€§ | ä¾èµ–è½®è¯¢é¢‘ç‡ | äº‹ä»¶é©±åŠ¨ï¼Œæ›´å¯é  |
| èµ„æºæ¶ˆè€— | é«˜ï¼ˆæŒç»­è½®è¯¢ï¼‰ | ä½ï¼ˆæŒ‰éœ€ï¼‰ |

## ä¸‰ã€æ‰¹é‡åˆ†æ CSV æ ¼å¼æ–¹æ¡ˆè®¾è®¡

### 3.1 CSV æ ¼å¼è®¾è®¡

**è€ƒè™‘å› ç´ **ï¼š
1. å­—æ®µå¯èƒ½åŒ…å«é€—å·ï¼ˆå¦‚ä¸­æ–‡ç¿»è¯‘ï¼‰
2. å­—æ®µå¯èƒ½åŒ…å«æ¢è¡Œç¬¦ï¼ˆå¦‚åˆ†æè§£é‡Šï¼‰
3. éœ€è¦è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
4. éœ€è¦ä¿è¯è§£æçš„å¥å£®æ€§

**æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨ RFC 4180 æ ‡å‡†çš„ CSV æ ¼å¼**

```csv
word,chinese_translation,pos_abbreviation,pos_english,pos_chinese,ipa,syllables,phonics_rule,analysis_explanation,frequency
"hello","ä½ å¥½","n.","noun","åè¯","/hÉ™ËˆloÊŠ/","hel-lo","å…ƒéŸ³å­—æ¯eåœ¨é—­éŸ³èŠ‚ä¸­å‘schwaéŸ³","The letter e in closed syllable makes a schwa sound",5
```

### 3.2 åç«¯æç¤ºè¯ä¿®æ”¹

```markdown
### ç¬¬ä¸‰æ­¥ï¼šè¾“å‡ºæ ¼å¼
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ CSV æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä½¿ç”¨ markdown ä»£ç å—æ ¼å¼ï¼Œç›´æ¥è¾“å‡ºçº¯ CSV æ–‡æœ¬ï¼š

```
word,chinese_translation,pos_abbreviation,pos_english,pos_chinese,ipa,syllables,phonics_rule,analysis_explanation,frequency
word1,translation1,pos_abbr1,pos_eng1,pos_chi1,ipa1,syllables1,rule1,explanation1,freq1
word2,translation2,pos_abbr2,pos_eng2,pos_chi2,ipa2,syllables2,rule2,explanation2,freq2
```

**é‡è¦æç¤º**ï¼š
1. æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹
2. å¦‚æœå­—æ®µå†…å®¹åŒ…å«åŒå¼•å·ï¼Œå¿…é¡»ç”¨ä¸¤ä¸ªåŒå¼•å·è½¬ä¹‰ï¼ˆä¾‹å¦‚ï¼š`""`ï¼‰
3. å¦‚æœå­—æ®µå†…å®¹åŒ…å«é€—å·ï¼Œå¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹
4. å¦‚æœå­—æ®µå†…å®¹åŒ…å«æ¢è¡Œç¬¦ï¼Œå¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹
5. ç©ºå­—æ®µä¹Ÿç”¨åŒå¼•å·è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼š`""`ï¼‰
```

### 3.3 åç«¯è§£æå®ç°

```rust
use csv::ReaderBuilder;
use std::io::Cursor;

/// è§£æ CSV æ ¼å¼çš„æ‰¹é‡åˆ†æå“åº”
fn parse_csv_response(
    &self,
    content: &str,
    logger: &Logger,
) -> Result<Vec<PhonicsWord>, Box<dyn std::error::Error>> {
    // æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ¼å¼
    let cleaned_content = self.clean_csv_markdown(content);
    
    // ä½¿ç”¨ csv crate è§£æ
    let cursor = Cursor::new(cleaned_content);
    let mut rdr = ReaderBuilder::new()
        .has_headers(true)  // ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        .flexible(true)       // å…è®¸å­—æ®µæ•°é‡ä¸ä¸€è‡´
        .from_reader(cursor);
    
    let mut words = Vec::new();
    
    for result in rdr.deserialize() {
        match result {
            Ok(record) => {
                // record ä¼šè‡ªåŠ¨è§£æä¸º PhonicsWordï¼ˆéœ€è¦å®ç° Deserializeï¼‰
                let word: PhonicsWord = record;
                if !word.word.is_empty() {
                    words.push(word);
                }
            }
            Err(e) => {
                logger.info("AI_SERVICE", &format!("âŒ CSV parsing error: {}", e));
                logger.info("AI_SERVICE", &format!("ğŸ“„ Response content: {}", content));
                return Err(format!("CSV parsing error: {}", e).into());
            }
        }
    }
    
    if words.is_empty() {
        return Err("No valid words found in CSV response".into());
    }
    
    Ok(words)
}
```

### 3.4 ä¾èµ–æ·»åŠ 

éœ€è¦åœ¨ `Cargo.toml` ä¸­æ·»åŠ ï¼š

```toml
[dependencies]
csv = "1.3"
```

### 3.5 é£é™©ç¼“è§£æªæ–½

| é£é™© | ç¼“è§£æªæ–½ |
|------|---------|
| å­—æ®µåŒ…å«é€—å· | ä½¿ç”¨åŒå¼•å·åŒ…è£¹æ‰€æœ‰å­—æ®µ |
| å­—æ®µåŒ…å«åŒå¼•å· | ä½¿ç”¨ä¸¤ä¸ªåŒå¼•å·è½¬ä¹‰ |
| å­—æ®µåŒ…å«æ¢è¡Œç¬¦ | ä½¿ç”¨åŒå¼•å·åŒ…è£¹ï¼Œcsv crate è‡ªåŠ¨å¤„ç† |
| å­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ | ä½¿ç”¨ `flexible(true)` å…è®¸çµæ´»è§£æ |
| ç¼–ç é—®é¢˜ | ç¡®ä¿ UTF-8 ç¼–ç  |
| å¤§é‡æ•°æ®å¯¼è‡´è¶…æ—¶ | ä½¿ç”¨æµå¼å¤„ç†ï¼Œä¸ç­‰å¾…å®Œæ•´å“åº” |

### 3.6 æ€§èƒ½å¯¹æ¯”

| æ–¹é¢ | JSON æ ¼å¼ | CSV æ ¼å¼ |
|------|-----------|---------|
| è¾“å‡ºå¤§å° | ~500 å­—èŠ‚/å•è¯ | ~300 å­—èŠ‚/å•è¯ |
| è§£æé€Ÿåº¦ | ä¸­ç­‰ | å¿« |
| ç½‘ç»œä¼ è¾“ | è¾ƒæ…¢ | è¾ƒå¿« |
| å¯è¯»æ€§ | é«˜ | ä¸­ |
| å¥å£®æ€§ | é«˜ | ä¸­ï¼ˆéœ€è¦æ­£ç¡®è½¬ä¹‰ï¼‰ |

## å››ã€å®æ–½å»ºè®®

### 4.1 ä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§**ï¼šå®ç°äº‹ä»¶æ¨é€æœºåˆ¶ï¼ˆè§£å†³è¿›åº¦ä¸æ›´æ–°çš„æ ¹æœ¬é—®é¢˜ï¼‰
2. **ä¸­ä¼˜å…ˆçº§**ï¼šæ‰¹é‡åˆ†æä½¿ç”¨ CSV æ ¼å¼ï¼ˆæé«˜æ€§èƒ½ï¼‰
3. **ä½ä¼˜å…ˆçº§**ï¼šå•è¯æå–ä½¿ç”¨ CSV æ ¼å¼ï¼ˆå·²ç»å®ç°ï¼‰

### 4.2 å®æ–½æ­¥éª¤

**äº‹ä»¶æ¨é€**ï¼š
1. å®šä¹‰äº‹ä»¶ç±»å‹ç»“æ„ä½“
2. ä¿®æ”¹ `analyze_words_parallel` æ·»åŠ  `AppHandle` å‚æ•°
3. åœ¨å…³é”®èŠ‚ç‚¹å‘é€äº‹ä»¶
4. å‰ç«¯æ·»åŠ äº‹ä»¶ç›‘å¬å™¨
5. ç§»é™¤è½®è¯¢é€»è¾‘
6. æµ‹è¯•äº‹ä»¶æ¨é€çš„å®æ—¶æ€§

**CSV æ ¼å¼**ï¼š
1. æ·»åŠ  `csv` ä¾èµ–
2. ä¿®æ”¹æ‰¹é‡åˆ†ææç¤ºè¯ä¸º CSV æ ¼å¼
3. å®ç° CSV è§£æé€»è¾‘
4. æ·»åŠ è½¬ä¹‰å’Œé”™è¯¯å¤„ç†
5. æµ‹è¯•å„ç§è¾¹ç•Œæƒ…å†µï¼ˆç‰¹æ®Šå­—ç¬¦ã€ç©ºå­—æ®µç­‰ï¼‰

### 4.3 å…¼å®¹æ€§è€ƒè™‘

- ä¿ç•™ `get_batch_analysis_progress` API ä½œä¸ºå¤‡ç”¨
- å‰ç«¯å¯ä»¥åŒæ—¶ä½¿ç”¨äº‹ä»¶æ¨é€å’Œè½®è¯¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
- å¦‚æœäº‹ä»¶æ¨é€å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°è½®è¯¢æœºåˆ¶
