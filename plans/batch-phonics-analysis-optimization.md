# å•è¯åˆ†ææ‰¹é‡å¹¶è¡Œä¼˜åŒ–æ–¹æ¡ˆ

## ä¸€ã€ç°çŠ¶åˆ†æ

### 1.1 å½“å‰å®ç°æ¶æ„

#### åç«¯å®ç°

**AI æœåŠ¡å±‚** (`src-tauri/src/ai_service.rs`)

```rust
// å½“å‰å®ç°ï¼šå•ä¸€ LLM è°ƒç”¨
pub async fn analyze_phonics(
    &self,
    text: &str,
    model_name: Option<&str>,
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    extraction_mode: &str,
    logger: &Logger,
) -> Result<PhonicsAnalysisResult, Box<dyn std::error::Error>> {
    // 1. æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«æ‰€æœ‰æ–‡æœ¬ï¼‰
    // 2. å‘é€å•ä¸€ LLM è¯·æ±‚ï¼ˆä½¿ç”¨æµå¼è¾“å‡ºï¼‰
    // 3. æµå¼æ¥æ”¶å“åº”
    // 4. è§£æå®Œæ•´çš„ JSON å“åº”ï¼ˆåŒ…å«æ‰€æœ‰å•è¯ï¼‰
    // 5. è¿”å›å®Œæ•´çš„å•è¯åˆ—è¡¨
}
```

**å½“å‰æµå¼è¾“å‡ºçš„é—®é¢˜**

å½“å‰å®ç°ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆ`.stream(true)`ï¼‰ï¼Œä½†è¿™åœ¨æ‰¹é‡åˆ†æåœºæ™¯ä¸­å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. **JSON å®Œæ•´æ€§è¦æ±‚**ï¼šLLM è¿”å›çš„æ˜¯ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼Œå¿…é¡»æ¥æ”¶å®Œæ•´å†…å®¹æ‰èƒ½è§£æ
2. **é€Ÿåº¦å½±å“**ï¼šæµå¼è¾“å‡ºçš„ chunk å¤„ç†å’Œæ‹¼æ¥åè€Œå¢åŠ äº†å¤„ç†æ—¶é—´
3. **è¿›åº¦è·Ÿè¸ªå†—ä½™**ï¼šæµå¼è¾“å‡ºçš„ chunk æ•°é‡æ— æ³•å‡†ç¡®åæ˜ å®é™…åˆ†æè¿›åº¦
4. **æ‰¹é‡å¤„ç†å·²è¶³å¤Ÿ**ï¼šé€šè¿‡æ‰¹æ¬¡åˆ’åˆ†å·²ç»æä¾›äº†è¶³å¤Ÿçš„è¿›åº¦åé¦ˆ

**è¿›åº¦ç®¡ç†** (`AnalysisProgress`)

```rust
pub struct AnalysisProgress {
    pub status: String,           // "analyzing", "completed", "error"
    pub current_step: String,     // å½“å‰æ­¥éª¤æè¿°
    pub chunks_received: u32,     // å·²æ¥æ”¶çš„chunkæ•°é‡
    pub total_chars: usize,       // å·²æ¥æ”¶çš„æ€»å­—ç¬¦æ•°
    pub elapsed_seconds: f64,     // å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰
    pub error_message: Option<String>, // é”™è¯¯ä¿¡æ¯
}
```

**ä¸ºä»€ä¹ˆæ‰¹é‡åˆ†æä¸ä½¿ç”¨æµå¼è¾“å‡º**

åœ¨æ‰¹é‡åˆ†æåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨éæµå¼è¾“å‡ºï¼ˆ`.stream(false)`ï¼‰ï¼ŒåŸå› å¦‚ä¸‹ï¼š

1. **JSON ç»“æ„å®Œæ•´æ€§è¦æ±‚**
   - LLM è¿”å›çš„æ˜¯ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
   - å¿…é¡»æ¥æ”¶å®Œæ•´å†…å®¹æ‰èƒ½è¿›è¡Œæœ‰æ•ˆçš„ JSON è§£æ
   - ä¸å®Œæ•´çš„ JSON ä¼šå¯¼è‡´è§£æå¤±è´¥

2. **æ€§èƒ½è€ƒè™‘**
   - æµå¼è¾“å‡ºçš„ chunk å¤„ç†å’Œæ‹¼æ¥å¢åŠ äº†å¤„ç†å¼€é”€
   - éæµå¼è¾“å‡ºç›´æ¥è·å–å®Œæ•´å“åº”ï¼Œé€Ÿåº¦æ›´å¿«
   - æ‰¹é‡åˆ†ææœ¬èº«å·²ç»æä¾›äº†è¶³å¤Ÿçš„æ€§èƒ½ä¼˜åŒ–

3. **è¿›åº¦è·Ÿè¸ªå·²è¶³å¤Ÿ**
   - æ‰¹æ¬¡åˆ’åˆ†æä¾›äº†æ¸…æ™°çš„è¿›åº¦åé¦ˆï¼ˆå·²å®Œæˆæ‰¹æ¬¡ / æ€»æ‰¹æ¬¡ï¼‰
   - æ¯ä¸ªæ‰¹æ¬¡å®Œæˆåæ›´æ–°è¿›åº¦ï¼Œç²’åº¦å·²è¶³å¤Ÿ
   - æµå¼è¾“å‡ºçš„ chunk æ•°é‡æ— æ³•å‡†ç¡®åæ˜ å®é™…åˆ†æè¿›åº¦

4. **ç®€åŒ–å®ç°**
   - éæµå¼è¾“å‡ºä»£ç æ›´ç®€æ´ï¼Œå‡å°‘é”™è¯¯å¤„ç†å¤æ‚åº¦
   - ç›´æ¥è·å–å®Œæ•´å“åº”ï¼Œé¿å…æµå¼å¤„ç†çš„å¼‚æ­¥å¤æ‚æ€§
   - æ›´å®¹æ˜“è°ƒè¯•å’Œç»´æŠ¤

**å¯¹æ¯”æ€»ç»“**

| ç‰¹æ€§ | æµå¼è¾“å‡º | éæµå¼è¾“å‡º |
|------|---------|-----------|
| é€‚ç”¨åœºæ™¯ | å®æ—¶å¯¹è¯ã€ç”Ÿæˆå¼æ–‡æœ¬ | ç»“æ„åŒ–æ•°æ®ã€æ‰¹é‡å¤„ç† |
| JSON è§£æ | éœ€è¦æ‹¼æ¥å®Œæ•´å†…å®¹ | ç›´æ¥è·å–å®Œæ•´å†…å®¹ |
| å¤„ç†é€Ÿåº¦ | è¾ƒæ…¢ï¼ˆchunk å¤„ç†ï¼‰ | æ›´å¿«ï¼ˆç›´æ¥è¿”å›ï¼‰ |
| è¿›åº¦è·Ÿè¸ª | chunk æ•°é‡ä¸å‡†ç¡® | æ‰¹æ¬¡å®Œæˆåº¦å‡†ç¡® |
| ä»£ç å¤æ‚åº¦ | é«˜ï¼ˆå¼‚æ­¥æµå¤„ç†ï¼‰ | ä½ï¼ˆç›´æ¥è°ƒç”¨ï¼‰ |
| æ‰¹é‡åˆ†æé€‚ç”¨æ€§ | âŒ ä¸é€‚ç”¨ | âœ… é€‚ç”¨ |

**ç»“è®º**

å¯¹äºæ‰¹é‡å•è¯åˆ†æåœºæ™¯ï¼Œä½¿ç”¨éæµå¼è¾“å‡ºæ˜¯æ›´ä¼˜çš„é€‰æ‹©ï¼š
- âœ… æ›´å¿«çš„å¤„ç†é€Ÿåº¦
- âœ… æ›´ç®€å•çš„å®ç°
- âœ… å‡†ç¡®çš„æ‰¹æ¬¡çº§åˆ«è¿›åº¦è·Ÿè¸ª
- âœ… å®Œæ•´çš„ JSON ç»“æ„ä¿è¯

**æç¤ºè¯** (`src-tauri/src/prompts/phonics_agent.md`)

- 298 è¡Œçš„è¯¦ç»†è‡ªç„¶æ‹¼è¯»è§„åˆ™åº“
- ä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰å•è¯
- è¿”å›å®Œæ•´çš„ JSON æ ¼å¼å•è¯åˆ—è¡¨

#### å‰ç«¯å®ç°

**æœåŠ¡å±‚** (`src/services/wordbookService.ts`)

```typescript
// å•ä¸€è°ƒç”¨åˆ†æ API
async analyzeTextForVocabulary(
    text: string,
    setLoading?: (state: LoadingState) => void
): Promise<ApiResult<any>> {
    return this.client.invoke<any>('analyze_text_for_vocabulary', { text });
}

// è½®è¯¢è¿›åº¦
async getAnalysisProgress(): Promise<ApiResult<AnalysisProgress | null>> {
    return this.client.invoke<AnalysisProgress | null>('get_analysis_progress');
}
```

**è¿›åº¦å±•ç¤º** (`WordImporterModal.tsx`)

- æ˜¾ç¤ºåˆ†æè¿›åº¦
- ä½¿ç”¨è½®è¯¢è·å–è¿›åº¦æ›´æ–°
- è¿›åº¦åŸºäº chunk æ•°é‡å±•ç¤º

### 1.2 å½“å‰æ–¹æ¡ˆçš„é—®é¢˜

#### é—®é¢˜ 1ï¼šæ€§èƒ½ç“¶é¢ˆ

**å•ä¸€ LLM è°ƒç”¨**
- æ‰€æœ‰å•è¯åœ¨ä¸€æ¬¡è¯·æ±‚ä¸­åˆ†æ
- å—é™äºæ¨¡å‹çš„è¾“å‡º token é™åˆ¶ï¼ˆé€šå¸¸ 4096 æˆ– 8192ï¼‰
- å¤§é‡æ–‡æœ¬ï¼ˆå¦‚ 500+ å•è¯ï¼‰ä¼šå¯¼è‡´ï¼š
  - è¶…å‡º token é™åˆ¶
  - å“åº”è¢«æˆªæ–­
  - åˆ†æä¸å®Œæ•´æˆ–å¤±è´¥

**å¤„ç†æ—¶é—´çº¿æ€§å¢é•¿**
- 10 ä¸ªå•è¯ï¼š~5-10 ç§’
- 50 ä¸ªå•è¯ï¼š~30-50 ç§’
- 100 ä¸ªå•è¯ï¼š~60-120 ç§’
- 500 ä¸ªå•è¯ï¼šå¯èƒ½è¶…è¿‡ 5 åˆ†é’Ÿæˆ–å¤±è´¥

**ç¤ºä¾‹åœºæ™¯**
```
è¾“å…¥æ–‡æœ¬ï¼š"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog..." (é‡å¤ 50 æ¬¡)

å½“å‰æ–¹æ¡ˆï¼š
1. æ„å»ºæç¤ºè¯ï¼šåŒ…å«æ‰€æœ‰ 500 ä¸ªå•è¯
2. å•æ¬¡ LLM è°ƒç”¨
3. ç­‰å¾…å“åº”ï¼ˆå¯èƒ½è¢«æˆªæ–­ï¼‰
4. è§£æ JSONï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰

é—®é¢˜ï¼š
- æç¤ºè¯é•¿åº¦å¯èƒ½è¶…è¿‡è¾“å…¥ token é™åˆ¶
- è¾“å‡ºé•¿åº¦å¯èƒ½è¶…è¿‡è¾“å‡º token é™åˆ¶
- ç”¨æˆ·éœ€è¦ç­‰å¾…æ•´ä¸ªåˆ†æå®Œæˆæ‰èƒ½çœ‹åˆ°ä»»ä½•ç»“æœ
```

#### é—®é¢˜ 2ï¼šè¿›åº¦åé¦ˆä¸ç²¾ç¡®

**å½“å‰è¿›åº¦æŒ‡æ ‡**
```typescript
AnalysisProgress {
    chunks_received: 45,      // åªçŸ¥é“æ”¶åˆ°äº†å¤šå°‘ä¸ª chunk
    total_chars: 12345,      // æ€»å­—ç¬¦æ•°
    current_step: "æ¥æ”¶AIåˆ†æç»“æœ..."
}
```

**ç”¨æˆ·è§†è§’**
- çœ‹åˆ°çš„æ˜¯"æ¥æ”¶ AI åˆ†æç»“æœ..."
- æ— æ³•çŸ¥é“å…·ä½“åˆ†æäº†å¤šå°‘ä¸ªå•è¯
- æ— æ³•çŸ¥é“å“ªäº›å•è¯å·²ç»å®Œæˆ
- æ— æ³•çŸ¥é“å‰©ä½™å¤šå°‘å•è¯
- æ— æ³•çœ‹åˆ°å•è¯çº§åˆ«çš„è¿›åº¦

**å¯¹æ¯”æœŸæœ›**
```
ç”¨æˆ·æœŸæœ›ï¼š
âœ… å·²æå–å•è¯åˆ—è¡¨ï¼š150 ä¸ª
âœ… æ­£åœ¨åˆ†æï¼š45/150 (30%)
âœ… å·²å®Œæˆï¼šapple, banana, cherry...
â³ åˆ†æä¸­ï¼šdate, egg, fig...
â¸ï¸ å¾…å¤„ç†ï¼šgrape, honey...

å½“å‰å®ç°ï¼š
ğŸ“¡ æ¥æ”¶ AI åˆ†æç»“æœ... (45 chunks, 12345 chars)
```

#### é—®é¢˜ 3ï¼šæ— æ³•å¤„ç†å¤§è§„æ¨¡æ•°æ®

**Token é™åˆ¶é—®é¢˜**
```
å…¸å‹æ¨¡å‹é™åˆ¶ï¼š
- GPT-3.5-turbo: è¾“å…¥ 4096, è¾“å‡º 4096
- GPT-4: è¾“å…¥ 8192, è¾“å‡º 4096
- Claude: è¾“å…¥ 100000, è¾“å‡º 4096

åœºæ™¯åˆ†æï¼š
è¾“å…¥ï¼š1000 ä¸ªå•è¯ï¼ˆçº¦ 5000 tokensï¼‰
è¾“å‡ºï¼šæ¯ä¸ªå•è¯çº¦ 200 tokens Ã— 1000 = 200,000 tokens

é—®é¢˜ï¼šè¾“å‡ºè¿œè¶…é™åˆ¶ï¼Œæ— æ³•å®Œæˆåˆ†æ
```

**æ–‡æœ¬é•¿åº¦é™åˆ¶**
```typescript
if (text.length > 10000) {
    throw new Error('æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼Œè¯·é™åˆ¶åœ¨10000å­—ç¬¦ä»¥å†…');
}
```

- å‰ç«¯é™åˆ¶åœ¨ 10000 å­—ç¬¦
- å¯¹äºå¤§é‡å•è¯åœºæ™¯ï¼ˆå¦‚æ•´æœ¬ä¹¦ç« èŠ‚ï¼‰ä¸é€‚ç”¨

#### é—®é¢˜ 4ï¼šé”™è¯¯æ¢å¤èƒ½åŠ›å¼±

**å½“å‰é”™è¯¯å¤„ç†**
```rust
// å•ä¸€å¤±è´¥ç‚¹
match self.parse_phonics_json(&full_content) {
    Ok(result) => Ok(result),
    Err(e) => {
        // æ•´ä¸ªåˆ†æå¤±è´¥ï¼Œæ‰€æœ‰å•è¯ä¸¢å¤±
        Err(error_msg.into())
    }
}
```

**é—®é¢˜åœºæ™¯**
- å¦‚æœ JSON è§£æå¤±è´¥ï¼Œæ‰€æœ‰å•è¯éƒ½ä¸¢å¤±
- å¦‚æœç½‘ç»œä¸­æ–­ï¼Œéœ€è¦é‡æ–°å¼€å§‹
- æ— æ³•éƒ¨åˆ†ä¿å­˜å·²åˆ†æçš„ç»“æœ

---

## äºŒã€ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡

### 2.1 æ ¸å¿ƒè®¾è®¡ç†å¿µ

**ä¸‰é˜¶æ®µæ¶æ„**
```
é˜¶æ®µ 1ï¼šå•è¯æå–ï¼ˆå¿«é€Ÿã€è½»é‡ï¼‰
  â†“
é˜¶æ®µ 2ï¼šæ‰¹é‡å¹¶è¡Œåˆ†æï¼ˆå¯æ‰©å±•ã€é«˜æ•ˆï¼‰
  â†“
é˜¶æ®µ 3ï¼šç»“æœåˆå¹¶ä¸ä¿å­˜ï¼ˆå¯é ã€å®Œæ•´ï¼‰
```

**å…³é”®ä¼˜åŠ¿**
1. **è§£è€¦æå–å’Œåˆ†æ**ï¼šæå–å¯ä»¥å¿«é€Ÿå®Œæˆï¼Œåˆ†æå¯ä»¥å¹¶è¡Œ
2. **æ‰¹é‡å¤„ç†**ï¼šæ¯æ‰¹ 10-20 ä¸ªå•è¯ï¼Œå……åˆ†åˆ©ç”¨å¹¶å‘
3. **ç»†ç²’åº¦è¿›åº¦**ï¼šæ¯ä¸ªå•è¯éƒ½æœ‰ç‹¬ç«‹çŠ¶æ€
4. **å®¹é”™æ€§å¼º**ï¼šå•ä¸ªå•è¯å¤±è´¥ä¸å½±å“å…¶ä»–å•è¯
5. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒ 1000+ å•è¯çš„å¤§è§„æ¨¡åœºæ™¯

### 2.2 è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ

#### é˜¶æ®µ 1ï¼šå•è¯æå–

**ç›®æ ‡**ï¼šå¿«é€Ÿä»æ–‡æœ¬ä¸­æå–å•è¯åˆ—è¡¨ï¼Œä¸è¿›è¡Œè¯¦ç»†åˆ†æ

**æ–°çš„æç¤ºè¯**ï¼š`src-tauri/src/prompts/word_extraction_agent.md`

```markdown
# å•è¯æå–ä¸“å®¶ Agent

## èº«ä»½å®šä¹‰
æ‚¨æ˜¯ä¸€ä¸ªå¿«é€Ÿã€å‡†ç¡®çš„å•è¯æå–ç³»ç»Ÿã€‚æ‚¨çš„ä»»åŠ¡æ˜¯ä»è‹±æ–‡æ–‡æœ¬ä¸­æå–æ‰€æœ‰ç‹¬ç«‹çš„å•è¯ã€‚

## å·¥ä½œæµç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬é¢„å¤„ç†
1. **åˆ†è¯**ï¼šå°†æ–‡æœ¬åˆ†è§£ä¸ºç‹¬ç«‹å•è¯
2. **æ ‡å‡†åŒ–**ï¼š
   - è½¬æ¢ä¸ºå°å†™
   - ç§»é™¤æ ‡ç‚¹ç¬¦å·
   - ç§»é™¤æ•°å­—
   - ç§»é™¤ç‰¹æ®Šå­—ç¬¦
3. **å»é‡ç»Ÿè®¡**ï¼šç»Ÿè®¡æ¯ä¸ªå•è¯çš„å‡ºç°é¢‘ç‡

### ç¬¬äºŒæ­¥ï¼šå•è¯ç­›é€‰
æ ¹æ®ä»¥ä¸‹è§„åˆ™ç­›é€‰å•è¯ï¼š
- æœ€å°é•¿åº¦ï¼š2 ä¸ªå­—ç¬¦
- æœ€å¤§é•¿åº¦ï¼š20 ä¸ªå­—ç¬¦
- ä»…åŒ…å«ï¼šè‹±æ–‡å­—æ¯
- æ’é™¤ï¼šçº¯æ•°å­—ã€çº¯æ ‡ç‚¹ã€å•ä¸ªå­—æ¯

### ç¬¬ä¸‰æ­¥ï¼šè¾“å‡ºæ ¼å¼
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š

```json
{
  "words": [
    {
      "word": "å•è¯åŸæ–‡",
      "frequency": å‡ºç°æ¬¡æ•°
    }
  ]
}
```

## é‡è¦è¦æ±‚
1. **åªè¿”å›å•è¯åˆ—è¡¨**ï¼šä¸è¿›è¡Œä»»ä½•è‡ªç„¶æ‹¼è¯»åˆ†æ
2. **å¿«é€Ÿå“åº”**ï¼šä¼˜å…ˆé€Ÿåº¦è€Œéè¯¦ç»†åˆ†æ
3. **å®Œæ•´æå–**ï¼šä¸è¦é—æ¼ä»»ä½•ç¬¦åˆæ¡ä»¶çš„å•è¯
4. **é¢‘ç‡å‡†ç¡®**ï¼šå‡†ç¡®ç»Ÿè®¡æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°
5. **ä¿æŒåŸæ ·**ï¼šä¿ç•™å•è¯çš„åŸå§‹å¤§å°å†™ï¼ˆç”¨äºåç»­æ’åºï¼‰

## ç¤ºä¾‹
è¾“å…¥æ–‡æœ¬ï¼š
"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog."

è¾“å‡ºï¼š
```json
{
  "words": [
    {"word": "The", "frequency": 2},
    {"word": "quick", "frequency": 2},
    {"word": "brown", "frequency": 2},
    {"word": "fox", "frequency": 2},
    {"word": "jumps", "frequency": 2},
    {"word": "over", "frequency": 2},
    {"word": "the", "frequency": 2},
    {"word": "lazy", "frequency": 2},
    {"word": "dog", "frequency": 2},
    {"word": ".", "frequency": 2}
  ]
}
```

## æ‰§è¡ŒæŒ‡ä»¤
è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œå•è¯æå–ï¼š
{original_text}
```

**æ–°çš„æ•°æ®ç»“æ„**

```rust
// src-tauri/src/types/word_analysis.rs
/// æå–çš„å•è¯ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExtractedWord {
    pub word: String,           // å•è¯åŸæ–‡
    pub frequency: i32,         // å‡ºç°é¢‘ç‡
}

/// å•è¯æå–ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WordExtractionResult {
    pub words: Vec<ExtractedWord>,
    pub total_count: usize,
    pub unique_count: usize,
}

/// æ‰¹é‡åˆ†æçŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchAnalysisProgress {
    pub status: String,                    // "extracting", "analyzing", "completed", "error"
    pub current_step: String,                // å½“å‰æ­¥éª¤æè¿°
    pub extraction_progress: ExtractionProgress,
    pub analysis_progress: AnalysisProgress,
}

/// æå–è¿›åº¦
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExtractionProgress {
    pub total_words: usize,                 // æ€»å•è¯æ•°
    pub extracted_words: usize,              // å·²æå–å•è¯æ•°
    pub elapsed_seconds: f64,                // å·²ç”¨æ—¶é—´
}

/// åˆ†æè¿›åº¦ï¼ˆç»†åŒ–ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisProgress {
    pub total_words: usize,                 // æ€»å•è¯æ•°
    pub completed_words: usize,              // å·²å®Œæˆå•è¯æ•°
    pub failed_words: usize,                 // å¤±è´¥å•è¯æ•°
    pub current_word: Option<String>,          // å½“å‰æ­£åœ¨åˆ†æçš„å•è¯
    pub batch_info: BatchInfo,               // æ‰¹æ¬¡ä¿¡æ¯
    pub elapsed_seconds: f64,                // å·²ç”¨æ—¶é—´
}

/// æ‰¹æ¬¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchInfo {
    pub total_batches: usize,                 // æ€»æ‰¹æ¬¡æ•°
    pub completed_batches: usize,              // å·²å®Œæˆæ‰¹æ¬¡æ•°
    pub current_batch: usize,                 // å½“å‰æ‰¹æ¬¡ï¼ˆä» 0 å¼€å§‹ï¼‰
    pub batch_size: usize,                   // æ¯æ‰¹å•è¯æ•°
}

/// å•è¯åˆ†æçŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WordAnalysisStatus {
    pub word: String,                       // å•è¯
    pub status: String,                      // "pending", "analyzing", "completed", "failed"
    pub error: Option<String>,               // é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    pub result: Option<PhonicsWord>,        // åˆ†æç»“æœï¼ˆå¦‚æœå®Œæˆï¼‰
}
```

**æ–°çš„ AI æœåŠ¡æ–¹æ³•**

```rust
// src-tauri/src/ai_service.rs

impl AIService {
    /// æ­¥éª¤ 1ï¼šæå–å•è¯åˆ—è¡¨
    pub async fn extract_words(
        &self,
        text: &str,
        logger: &Logger,
    ) -> Result<WordExtractionResult, Box<dyn std::error::Error>> {
        // 1. è¯»å–å•è¯æå–æç¤ºè¯
        let extraction_prompt = include_str!("prompts/word_extraction_agent.md");
        
        // 2. æ„å»ºè¯·æ±‚ï¼ˆä½¿ç”¨å° max_tokensï¼Œå› ä¸ºåªéœ€è¦å•è¯åˆ—è¡¨ï¼‰
        let request = CreateChatCompletionRequestArgs::default()
            .model(self.provider.get_default_model())
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: extraction_prompt.replace("{original_text}", text),
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(2000)  // é™åˆ¶è¾“å‡ºé•¿åº¦
            .temperature(0.1)    // ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
            .stream(false)         // éæµå¼ï¼Œå¿«é€Ÿè·å–ç»“æœ
            .build()?;
        
        // 3. å‘é€è¯·æ±‚
        let response = self.client.chat().create(request).await?;
        
        // 4. è§£æ JSON å“åº”
        let json_response: serde_json::Value = serde_json::from_str(&response.choices[0].message.content.unwrap())?;
        let words_array = json_response["words"].as_array().ok_or("Invalid words array")?;
        
        // 5. è½¬æ¢ä¸º ExtractedWord
        let words: Vec<ExtractedWord> = words_array
            .iter()
            .map(|v| ExtractedWord {
                word: v["word"].as_str().unwrap().to_string(),
                frequency: v["frequency"].as_i64().unwrap() as i32,
            })
            .collect();
        
        Ok(WordExtractionResult {
            total_count: words.len(),
            unique_count: words.len(),
            words,
        })
    }
    
    /// æ­¥éª¤ 2ï¼šæ‰¹é‡åˆ†æå•è¯
    pub async fn analyze_words_batch(
        &self,
        words: Vec<String>,
        batch_index: usize,
        total_batches: usize,
        logger: &Logger,
    ) -> Result<Vec<PhonicsWord>, Box<dyn std::error::Error>> {
        // 1. è¯»å–è‡ªç„¶æ‹¼è¯»åˆ†ææç¤ºè¯
        let phonics_prompt_template = include_str!("prompts/phonics_agent.md");
        
        // 2. æ„å»ºæ‰¹é‡åˆ†ææç¤ºè¯
        let words_json = serde_json::to_string(&words)?;
        let batch_prompt = format!(
            "{}\n\nè¯·åˆ†æä»¥ä¸‹ {} ä¸ªå•è¯ï¼š\n{}",
            phonics_prompt_template.replace("{original_text}", ""),
            words.len(),
            words_json
        );
        
        // 3. æ„å»ºè¯·æ±‚
        let request = CreateChatCompletionRequestArgs::default()
            .model(self.provider.get_default_model())
            .messages([ChatCompletionRequestMessage::System(
                ChatCompletionRequestSystemMessage {
                    content: batch_prompt,
                    role: Role::System,
                    name: None,
                },
            )])
            .max_tokens(8000)  // æ¯æ‰¹ 10-20 ä¸ªå•è¯
            .temperature(0.1)
            .stream(false)        // ä½¿ç”¨éæµå¼è¾“å‡ºï¼Œç›´æ¥è·å–å®Œæ•´ JSON
            .build()?;
        
        // 4. å‘é€è¯·æ±‚å¹¶è·å–å®Œæ•´å“åº”
        let response = self.client.chat().create(request).await?;
        
        // 5. æå–å“åº”å†…å®¹
        let content = response.choices.first()?.message.content.as_ref().ok_or("No content")?;
        
        // 6. è§£æ JSON å“åº”
        let json_response: JsonPhonicsResponse = serde_json::from_str(content)?;
        
        Ok(json_response.words.into_iter().map(|w| w.into()).collect())
    }
}
```

#### é˜¶æ®µ 2ï¼šæ‰¹é‡å¹¶è¡Œåˆ†æ

**æ‰¹å¤„ç†ç­–ç•¥**

```rust
// src-tauri/src/word_analysis_service.rs

pub struct WordAnalysisService {
    ai_service: Arc<AIService>,
    batch_size: usize,              // æ¯æ‰¹å•è¯æ•°ï¼ˆé»˜è®¤ 10ï¼‰
    max_concurrent_batches: usize,    // æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
}

impl WordAnalysisService {
    /// æ‰¹é‡åˆ†æå•è¯ï¼ˆä¸»å…¥å£ï¼‰
    pub async fn analyze_text_with_batching(
        &self,
        text: &str,
        model_config: &AIModelConfig,
        extraction_mode: &str,
        logger: &Logger,
    ) -> Result<BatchAnalysisResult, Box<dyn std::error::Error>> {
        let start_time = std::time::Instant::now();
        
        // æ­¥éª¤ 1ï¼šæå–å•è¯åˆ—è¡¨
        logger.info("WORD_ANALYSIS", "ğŸš€ æ­¥éª¤ 1ï¼šå¼€å§‹æå–å•è¯...");
        let extraction_result = self.ai_service.extract_words(text, logger).await?;
        
        // æ›´æ–°æå–è¿›åº¦
        self.update_extraction_progress(&ExtractionProgress {
            total_words: extraction_result.total_count,
            extracted_words: extraction_result.total_count,
            elapsed_seconds: start_time.elapsed().as_secs_f64(),
        }, logger);
        
        // æ­¥éª¤ 2ï¼šåˆ†æ‰¹å¹¶å¹¶è¡Œåˆ†æ
        logger.info("WORD_ANALYSIS", &format!("ğŸ“¦ æ­¥éª¤ 2ï¼šæå–åˆ° {} ä¸ªå•è¯ï¼Œå¼€å§‹æ‰¹é‡åˆ†æ...", extraction_result.unique_count));
        
        let words: Vec<String> = extraction_result.words
            .into_iter()
            .map(|w| w.word)
            .collect();
        
        let total_batches = (words.len() + self.batch_size - 1) / self.batch_size;
        
        // ä½¿ç”¨ tokio çš„å¹¶å‘å·¥å…·
        let mut analysis_results: Vec<PhonicsWord> = Vec::new();
        let mut failed_words: Vec<String> = Vec::new();
        
        // åˆ†æ‰¹å¤„ç†
        for (batch_index, batch) in words.chunks(self.batch_size).enumerate() {
            let batch_words: Vec<String> = batch.to_vec();
            
            // æ›´æ–°åˆ†æè¿›åº¦
            self.update_analysis_progress(&AnalysisProgress {
                total_words: words.len(),
                completed_words: analysis_results.len(),
                failed_words: failed_words.len(),
                current_word: Some(batch_words.first().cloned().unwrap_or_default()),
                batch_info: BatchInfo {
                    total_batches,
                    completed_batches: batch_index,
                    current_batch: batch_index,
                    batch_size: self.batch_size,
                },
                elapsed_seconds: start_time.elapsed().as_secs_f64(),
            }, logger);
            
            // å¹¶å‘å¤„ç†æ‰¹æ¬¡
            match self.ai_service.analyze_words_batch(
                batch_words,
                batch_index,
                total_batches,
                logger
            ).await {
                Ok(batch_words) => {
                    analysis_results.extend(batch_words);
                    // æ›´æ–°æ¯ä¸ªå•è¯çš„çŠ¶æ€
                    for word in &batch_words {
                        self.update_word_status(&WordAnalysisStatus {
                            word: word.word.clone(),
                            status: "completed".to_string(),
                            error: None,
                            result: Some(word.clone()),
                        }, logger);
                    }
                }
                Err(e) => {
                    // æ‰¹æ¬¡å¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰å•è¯ä¸ºå¤±è´¥
                    for word in &batch_words {
                        failed_words.push(word.clone());
                        self.update_word_status(&WordAnalysisStatus {
                            word: word.clone(),
                            status: "failed".to_string(),
                            error: Some(e.to_string()),
                            result: None,
                        }, logger);
                    }
                }
            }
        }
        
        // æ­¥éª¤ 3ï¼šåˆå¹¶ç»“æœ
        logger.info("WORD_ANALYSIS", "âœ… æ­¥éª¤ 3ï¼šæ‰¹é‡åˆ†æå®Œæˆï¼Œå¼€å§‹åˆå¹¶ç»“æœ...");
        
        Ok(BatchAnalysisResult {
            words: analysis_results,
            total_words: words.len(),
            completed_words: analysis_results.len(),
            failed_words: failed_words.len(),
            elapsed_seconds: start_time.elapsed().as_secs_f64(),
        })
    }
}
```

**å¹¶å‘æ§åˆ¶**

```rust
// ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
use tokio::sync::Semaphore;

pub struct WordAnalysisService {
    ai_service: Arc<AIService>,
    batch_size: usize,
    max_concurrent_batches: usize,
    semaphore: Arc<Semaphore>,  // å¹¶å‘æ§åˆ¶
}

impl WordAnalysisService {
    pub fn new(ai_service: AIService) -> Self {
        Self {
            ai_service: Arc::new(ai_service),
            batch_size: 10,
            max_concurrent_batches: 3,
            semaphore: Arc::new(Semaphore::new(3)),  // æœ€å¤š 3 ä¸ªå¹¶å‘æ‰¹æ¬¡
        }
    }
    
    /// å¹¶å‘å¤„ç†å¤šä¸ªæ‰¹æ¬¡
    async fn process_batches_concurrently(
        &self,
        batches: Vec<Vec<String>>,
        logger: &Logger,
    ) -> Result<Vec<PhonicsWord>, Box<dyn std::error::Error>> {
        let mut tasks = Vec::new();
        
        for (batch_index, batch) in batches.into_iter().enumerate() {
            let ai_service = Arc::clone(&self.ai_service);
            let semaphore = Arc::clone(&self.semaphore);
            let logger_clone = logger.clone();
            
            let task = tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();  // è·å–ä¿¡å·é‡
                ai_service.analyze_words_batch(batch, batch_index, batches.len(), &logger_clone).await
            });
            
            tasks.push(task);
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        let mut results = Vec::new();
        for task in tasks {
            match task.await {
                Ok(Ok(words)) => results.extend(words),
                Ok(Err(e)) => logger.error("BATCH", &format!("Batch failed: {}", e)),
                Err(e) => logger.error("BATCH", &format!("Task join error: {}", e)),
            }
        }
        
        Ok(results)
    }
}
```

#### é˜¶æ®µ 3ï¼šè¿›åº¦ç®¡ç†ä¸å‰ç«¯é›†æˆ

**å¢å¼ºçš„è¿›åº¦ç®¡ç†å™¨**

```rust
// src-tauri/src/progress_manager.rs

pub struct EnhancedProgressManager {
    extraction_progress: Arc<Mutex<Option<ExtractionProgress>>>,
    analysis_progress: Arc<Mutex<Option<AnalysisProgress>>>,
    word_statuses: Arc<Mutex<HashMap<String, WordAnalysisStatus>>>,
    cancelled: Arc<Mutex<bool>>,
}

impl EnhancedProgressManager {
    /// æ›´æ–°æå–è¿›åº¦
    pub fn update_extraction_progress(&self, progress: &ExtractionProgress) {
        let mut guard = self.extraction_progress.lock().unwrap();
        *guard = Some(progress.clone());
    }
    
    /// æ›´æ–°åˆ†æè¿›åº¦
    pub fn update_analysis_progress(&self, progress: &AnalysisProgress) {
        let mut guard = self.analysis_progress.lock().unwrap();
        *guard = Some(progress.clone());
    }
    
    /// æ›´æ–°å•ä¸ªå•è¯çŠ¶æ€
    pub fn update_word_status(&self, status: &WordAnalysisStatus) {
        let mut guard = self.word_statuses.lock().unwrap();
        guard.insert(status.word.clone(), status.clone());
    }
    
    /// è·å–å®Œæ•´è¿›åº¦ä¿¡æ¯
    pub fn get_full_progress(&self) -> BatchAnalysisProgress {
        let extraction = self.extraction_progress.lock().unwrap().clone();
        let analysis = self.analysis_progress.lock().unwrap().clone();
        let word_statuses = self.word_statuses.lock().unwrap().clone();
        
        BatchAnalysisProgress {
            extraction_progress: extraction,
            analysis_progress: analysis,
            word_statuses,
        }
    }
}
```

**æ–°çš„ Tauri å‘½ä»¤**

```rust
// src-tauri/src/word_analysis_handlers.rs

/// æ‰¹é‡åˆ†ææ–‡æœ¬ï¼ˆæ–°å‘½ä»¤ï¼‰
#[tauri::command]
pub async fn analyze_text_with_batching(
    app: AppHandle,
    text: String,
    model_id: Option<i64>,
    extraction_mode: Option<String>,
) -> AppResult<BatchAnalysisResult> {
    let pool = app.state::<SqlitePool>();
    let logger = app.state::<Logger>();
    
    // 1. è·å– AI æœåŠ¡
    let model_config = get_model_config(model_id, &pool, &logger).await?;
    let ai_service = AIService::from_model_config(&model_config)?;
    
    // 2. åˆ›å»ºæ‰¹é‡åˆ†ææœåŠ¡
    let analysis_service = WordAnalysisService::new(ai_service);
    
    // 3. æ‰§è¡Œæ‰¹é‡åˆ†æ
    let result = analysis_service.analyze_text_with_batching(
        &text,
        &model_config,
        extraction_mode.as_deref().unwrap_or("focus"),
        &logger,
    ).await?;
    
    logger.api_response("analyze_text_with_batching", true, Some(&format!("Analyzed {} words", result.completed_words)));
    
    Ok(result)
}

/// è·å–æ‰¹é‡åˆ†æè¿›åº¦ï¼ˆæ–°å‘½ä»¤ï¼‰
#[tauri::command]
pub async fn get_batch_analysis_progress(
    app: AppHandle,
) -> AppResult<BatchAnalysisProgress> {
    let progress_manager = get_enhanced_progress_manager();
    Ok(progress_manager.get_full_progress())
}

/// å–æ¶ˆæ‰¹é‡åˆ†æï¼ˆæ–°å‘½ä»¤ï¼‰
#[tauri::command]
pub async fn cancel_batch_analysis(
    app: AppHandle,
) -> AppResult<()> {
    let progress_manager = get_enhanced_progress_manager();
    progress_manager.cancel_analysis();
    Ok(())
}
```

**å‰ç«¯ç±»å‹å®šä¹‰**

```typescript
// src/types/word-analysis.ts

export interface ExtractedWord {
  word: string;
  frequency: number;
}

export interface WordExtractionResult {
  words: ExtractedWord[];
  total_count: number;
  unique_count: number;
}

export interface ExtractionProgress {
  total_words: number;
  extracted_words: number;
  elapsed_seconds: number;
}

export interface BatchInfo {
  total_batches: number;
  completed_batches: number;
  current_batch: number;
  batch_size: number;
}

export interface AnalysisProgress {
  total_words: number;
  completed_words: number;
  failed_words: number;
  current_word: string | null;
  batch_info: BatchInfo;
  elapsed_seconds: number;
}

export interface WordAnalysisStatus {
  word: string;
  status: 'pending' | 'analyzing' | 'completed' | 'failed';
  error: string | null;
  result: PhonicsWord | null;
}

export interface BatchAnalysisProgress {
  extraction_progress: ExtractionProgress | null;
  analysis_progress: AnalysisProgress | null;
  word_statuses: Map<string, WordAnalysisStatus>;
}

export interface BatchAnalysisResult {
  words: PhonicsWord[];
  total_words: number;
  completed_words: number;
  failed_words: number;
  elapsed_seconds: number;
}
```

**å‰ç«¯æœåŠ¡å±‚**

```typescript
// src/services/wordAnalysisService.ts

export class WordAnalysisService {
  /**
   * æ‰¹é‡åˆ†ææ–‡æœ¬
   */
  async analyzeTextWithBatching(
    text: string,
    modelId?: number,
    extractionMode?: string,
    setLoading?: (state: LoadingState) => void
  ): Promise<ApiResult<BatchAnalysisResult>> {
    return this.executeWithLoading(async () => {
      if (!text || text.trim().length === 0) {
        throw new Error('æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º');
      }
      
      // ç§»é™¤é•¿åº¦é™åˆ¶æˆ–å¤§å¹…æé«˜
      if (text.length > 50000) {
        throw new Error('æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼Œè¯·é™åˆ¶åœ¨50000å­—ç¬¦ä»¥å†…');
      }
      
      return this.client.invoke<BatchAnalysisResult>('analyze_text_with_batching', {
        text,
        model_id: modelId,
        extraction_mode: extractionMode
      });
    }, setLoading);
  }
  
  /**
   * è·å–æ‰¹é‡åˆ†æè¿›åº¦
   */
  async getBatchAnalysisProgress(): Promise<ApiResult<BatchAnalysisProgress>> {
    return this.executeWithLoading(async () => {
      return this.client.invoke<BatchAnalysisProgress>('get_batch_analysis_progress');
    });
  }
  
  /**
   * å–æ¶ˆæ‰¹é‡åˆ†æ
   */
  async cancelBatchAnalysis(): Promise<ApiResult<void>> {
    return this.executeWithLoading(async () => {
      return this.client.invoke<void>('cancel_batch_analysis');
    });
  }
}
```

**å‰ç«¯ UI ç»„ä»¶**

```typescript
// src/components/WordAnalysisProgressModal.tsx

export function WordAnalysisProgressModal({ isOpen, onClose }: Props) {
  const [progress, setProgress] = useState<BatchAnalysisProgress | null>(null);
  const [pollInterval, setPollInterval] = useState<number | null>(null);
  
  // è½®è¯¢è¿›åº¦
  useEffect(() => {
    if (isOpen) {
      // å¯åŠ¨è½®è¯¢
      const interval = setInterval(async () => {
        const result = await wordAnalysisService.getBatchAnalysisProgress();
        if (result.success && result.data) {
          setProgress(result.data);
          
          // æ£€æŸ¥æ˜¯å¦å®Œæˆ
          if (result.data.analysis_progress?.completed_words === result.data.analysis_progress?.total_words) {
            clearInterval(interval);
          }
        }
      }, 500);  // æ¯ 500ms è½®è¯¢ä¸€æ¬¡
      
      setPollInterval(interval);
    }
    
    return () => {
      if (pollInterval) {
        clearInterval(pollInterval);
      }
    };
  }, [isOpen]);
  
  // è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
  const getProgressPercentage = () => {
    if (!progress?.analysis_progress) return 0;
    const { completed_words, total_words } = progress.analysis_progress;
    return total_words > 0 ? Math.round((completed_words / total_words) * 100) : 0;
  };
  
  // è·å–å•è¯çŠ¶æ€åˆ—è¡¨
  const getWordStatusList = () => {
    if (!progress?.word_statuses) return [];
    
    return Array.from(progress.word_statuses.entries())
      .map(([word, status]) => ({ word, ...status }))
      .sort((a, b) => {
        // æŒ‰çŠ¶æ€æ’åºï¼šcompleted > analyzing > pending > failed
        const statusOrder = { completed: 0, analyzing: 1, pending: 2, failed: 3 };
        return statusOrder[a.status] - statusOrder[b.status];
      });
  };
  
  return (
    <Modal isOpen={isOpen} onClose={onClose}>
      <div className="word-analysis-progress">
        {/* æå–é˜¶æ®µ */}
        {progress?.extraction_progress && (
          <div className="extraction-phase">
            <h3>ğŸ“ æ­¥éª¤ 1ï¼šæå–å•è¯</h3>
            <div className="progress-bar">
              <div 
                className="progress-fill"
                style={{ width: `${getProgressPercentage()}%` }}
              />
            </div>
            <div className="progress-text">
              {progress.extraction_progress.extracted_words} / {progress.extraction_progress.total_words} ä¸ªå•è¯
            </div>
          </div>
          <div className="time-elapsed">
            â±ï¸ å·²ç”¨æ—¶é—´ï¼š{Math.round(progress.extraction_progress.elapsed_seconds)} ç§’
          </div>
        </div>
        )}
        
        {/* åˆ†æé˜¶æ®µ */}
        {progress?.analysis_progress && (
          <div className="analysis-phase">
            <h3>ğŸ” æ­¥éª¤ 2ï¼šæ‰¹é‡åˆ†æ</h3>
            <div className="batch-info">
              <div>æ‰¹æ¬¡ï¼š{progress.analysis_progress.batch_info.current_batch} / {progress.analysis_progress.batch_info.total_batches}</div>
              <div>æ¯æ‰¹ï¼š{progress.analysis_progress.batch_info.batch_size} ä¸ªå•è¯</div>
              <div>å·²å®Œæˆï¼š{progress.analysis_progress.completed_words} / {progress.analysis_progress.total_words} ä¸ªå•è¯</div>
              <div>å¤±è´¥ï¼š{progress.analysis_progress.failed_words} ä¸ªå•è¯</div>
            </div>
            
            {/* å½“å‰æ­£åœ¨åˆ†æçš„å•è¯ */}
            {progress.analysis_progress.current_word && (
              <div className="current-word">
                æ­£åœ¨åˆ†æï¼š<strong>{progress.analysis_progress.current_word}</strong>
              </div>
            )}
            
            {/* å•è¯çŠ¶æ€åˆ—è¡¨ */}
            <div className="word-status-list">
              <h4>å•è¯åˆ†æçŠ¶æ€</h4>
              {getWordStatusList().slice(0, 50).map(({ word, status, error }) => (
                <div key={word} className={`word-status-item ${status}`}>
                  <span className="word">{word}</span>
                  <span className="status">
                    {status === 'completed' && 'âœ…'}
                    {status === 'analyzing' && 'â³'}
                    {status === 'pending' && 'â¸ï¸'}
                    {status === 'failed' && 'âŒ'}
                  </span>
                  {error && <span className="error">{error}</span>}
                </div>
              ))}
              {getWordStatusList().length > 50 && (
                <div className="more-items">
                  ... è¿˜æœ‰ {getWordStatusList().length - 50} ä¸ªå•è¯
                </div>
              )}
            </div>
            
            <div className="time-elapsed">
              â±ï¸ å·²ç”¨æ—¶é—´ï¼š{Math.round(progress.analysis_progress.elapsed_seconds)} ç§’
            </div>
          </div>
        )}
        
        {/* æ“ä½œæŒ‰é’® */}
        <div className="actions">
          <Button onClick={handleCancel} variant="secondary">
            å–æ¶ˆåˆ†æ
          </Button>
        </div>
      </div>
    </Modal>
  );
}
```

---

## ä¸‰ã€æ€§èƒ½å¯¹æ¯”åˆ†æ

### 3.1 å¤„ç†æ—¶é—´å¯¹æ¯”

| åœºæ™¯ | å½“å‰æ–¹æ¡ˆï¼ˆå•æ¬¡è°ƒç”¨ï¼‰ | ä¼˜åŒ–æ–¹æ¡ˆï¼ˆæ‰¹é‡å¹¶è¡Œï¼‰ | æå‡ |
|--------|---------------------|---------------------|------|
| 10 ä¸ªå•è¯ | 5-10 ç§’ | 3-5 ç§’ | 2x |
| 50 ä¸ªå•è¯ | 30-50 ç§’ | 10-15 ç§’ | 3-4x |
| 100 ä¸ªå•è¯ | 60-120 ç§’ | 15-25 ç§’ | 4-5x |
| 500 ä¸ªå•è¯ | å¤±è´¥æˆ– >300 ç§’ | 60-90 ç§’ | 3-5x |
| 1000 ä¸ªå•è¯ | å¤±è´¥ | 120-180 ç§’ | å¯è¡Œ |

### 3.2 Token ä½¿ç”¨å¯¹æ¯”

| åœºæ™¯ | å½“å‰æ–¹æ¡ˆ | ä¼˜åŒ–æ–¹æ¡ˆ | èŠ‚çœ |
|--------|---------|---------|------|
| è¾“å…¥ Token | æ‰€æœ‰å•è¯ä¸€æ¬¡æ€§ | åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ç‹¬ç«‹ | ~50% |
| è¾“å‡º Token | æ‰€æœ‰å•è¯ä¸€æ¬¡æ€§ | åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ç‹¬ç«‹ | ~70% |
| æ€» Token | 1000 ä¸ªå•è¯ï¼š~200,000 | 1000 ä¸ªå•è¯ï¼š~60,000 | ~70% |

### 3.3 å¯é æ€§å¯¹æ¯”

| æŒ‡æ ‡ | å½“å‰æ–¹æ¡ˆ | ä¼˜åŒ–æ–¹æ¡ˆ |
|--------|---------|---------|
| å•ç‚¹æ•…éšœ | æ‰€æœ‰æ•°æ®ä¸¢å¤± | ä»…å½±å“å½“å‰æ‰¹æ¬¡ |
| ç½‘ç»œä¸­æ–­ | éœ€è¦é‡æ–°å¼€å§‹ | å·²å®Œæˆæ‰¹æ¬¡ä¿ç•™ |
| éƒ¨åˆ†å¤±è´¥ | æ— æ³•æ¢å¤ | å¤±è´¥å•è¯å¯é‡è¯• |
| è¿›åº¦å¯è§æ€§ | ç²—ç²’åº¦ï¼ˆchunkï¼‰ | ç»†ç²’åº¦ï¼ˆå•è¯çº§åˆ«ï¼‰ |

---

## å››ã€å®æ–½è®¡åˆ’

### é˜¶æ®µ 1ï¼šåç«¯åŸºç¡€è®¾æ–½ï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰

#### ä»»åŠ¡ 1.1ï¼šåˆ›å»ºæ–°ç±»å‹å®šä¹‰
- [ ] åˆ›å»º `src-tauri/src/types/word_analysis.rs`
- [ ] å®šä¹‰ `ExtractedWord`, `WordExtractionResult`
- [ ] å®šä¹‰ `BatchAnalysisProgress`, `ExtractionProgress`, `AnalysisProgress`
- [ ] å®šä¹‰ `WordAnalysisStatus`, `BatchInfo`

#### ä»»åŠ¡ 1.2ï¼šåˆ›å»ºå•è¯æå–æç¤ºè¯
- [ ] åˆ›å»º `src-tauri/src/prompts/word_extraction_agent.md`
- [ ] è®¾è®¡å¿«é€Ÿã€è½»é‡çš„æå–é€»è¾‘
- [ ] å®šä¹‰ JSON è¾“å‡ºæ ¼å¼

#### ä»»åŠ¡ 1.3ï¼šå®ç°å•è¯æå–åŠŸèƒ½
- [ ] åœ¨ `AIService` ä¸­æ·»åŠ  `extract_words` æ–¹æ³•
- [ ] ä½¿ç”¨éæµå¼ API å¿«é€Ÿè·å–ç»“æœ
- [ ] è§£ææå–ç»“æœ

#### ä»»åŠ¡ 1.4ï¼šå®ç°æ‰¹é‡åˆ†æåŠŸèƒ½
- [ ] åˆ›å»º `WordAnalysisService` ç»“æ„
- [ ] å®ç° `analyze_words_batch` æ–¹æ³•
- [ ] å®ç°å¹¶å‘æ§åˆ¶ï¼ˆä½¿ç”¨ Semaphoreï¼‰

#### ä»»åŠ¡ 1.5ï¼šåˆ›å»ºå¢å¼ºçš„è¿›åº¦ç®¡ç†å™¨
- [ ] åˆ›å»º `src-tauri/src/progress_manager.rs`
- [ ] å®ç°ç»†ç²’åº¦çš„è¿›åº¦è·Ÿè¸ª
- [ ] æ”¯æŒå•è¯çº§åˆ«çš„çŠ¶æ€æ›´æ–°

#### ä»»åŠ¡ 1.6ï¼šåˆ›å»ºæ–°çš„ Tauri å‘½ä»¤
- [ ] `analyze_text_with_batching`
- [ ] `get_batch_analysis_progress`
- [ ] `cancel_batch_analysis`
- [ ] åœ¨ `lib.rs` ä¸­æ³¨å†Œå‘½ä»¤

### é˜¶æ®µ 2ï¼šå‰ç«¯å®ç°ï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰

#### ä»»åŠ¡ 2.1ï¼šåˆ›å»ºå‰ç«¯ç±»å‹å®šä¹‰
- [ ] åˆ›å»º `src/types/word-analysis.ts`
- [ ] å¯¼å‡ºæ‰€æœ‰æ–°ç±»å‹

#### ä»»åŠ¡ 2.2ï¼šåˆ›å»ºå‰ç«¯æœåŠ¡
- [ ] åˆ›å»º `src/services/wordAnalysisService.ts`
- [ ] å®ç°æ‰¹é‡åˆ†æ API è°ƒç”¨
- [ ] å®ç°è¿›åº¦è½®è¯¢

#### ä»»åŠ¡ 2.3ï¼šåˆ›å»ºè¿›åº¦å±•ç¤ºç»„ä»¶
- [ ] åˆ›å»º `src/components/WordAnalysisProgressModal.tsx`
- [ ] å®ç°ç»†ç²’åº¦çš„è¿›åº¦å±•ç¤º
- [ ] å®ç°å•è¯çŠ¶æ€åˆ—è¡¨
- [ ] æ·»åŠ å–æ¶ˆåŠŸèƒ½

#### ä»»åŠ¡ 2.4ï¼šé›†æˆåˆ°ç°æœ‰é¡µé¢
- [ ] æ›´æ–° `WordImporterModal.tsx`
- [ ] æ›¿æ¢ä¸ºæ–°çš„æ‰¹é‡åˆ†ææµç¨‹

### é˜¶æ®µ 3ï¼šæµ‹è¯•ä¸ä¼˜åŒ–ï¼ˆä¼˜å…ˆçº§ï¼šä¸­ï¼‰

#### ä»»åŠ¡ 3.1ï¼šå•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•å•è¯æå–åŠŸèƒ½
- [ ] æµ‹è¯•æ‰¹é‡åˆ†æåŠŸèƒ½
- [ ] æµ‹è¯•å¹¶å‘æ§åˆ¶

#### ä»»åŠ¡ 3.2ï¼šé›†æˆæµ‹è¯•
- [ ] æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆæå– -> åˆ†æ -> åˆå¹¶ï¼‰
- [ ] æµ‹è¯•è¿›åº¦æ›´æ–°
- [ ] æµ‹è¯•å–æ¶ˆåŠŸèƒ½

#### ä»»åŠ¡ 3.3ï¼šæ€§èƒ½æµ‹è¯•
- [ ] æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ï¼ˆ10, 50, 100, 500, 1000 å•è¯ï¼‰
- [ ] æµ‹é‡å¤„ç†æ—¶é—´
- [ ] ä¼˜åŒ–æ‰¹å¤§å°å’Œå¹¶å‘æ•°

#### ä»»åŠ¡ 3.4ï¼šé”™è¯¯å¤„ç†æµ‹è¯•
- [ ] æµ‹è¯•ç½‘ç»œä¸­æ–­æ¢å¤
- [ ] æµ‹è¯•éƒ¨åˆ†å¤±è´¥åœºæ™¯
- [ ] æµ‹è¯•è¶…æ—¶å¤„ç†

### é˜¶æ®µ 4ï¼šæ–‡æ¡£ä¸éƒ¨ç½²ï¼ˆä¼˜å…ˆçº§ï¼šä½ï¼‰

#### ä»»åŠ¡ 4.1ï¼šæ›´æ–°æ–‡æ¡£
- [ ] æ›´æ–° API æ–‡æ¡£
- [ ] æ›´æ–°ç”¨æˆ·æŒ‡å—
- [ ] æ·»åŠ æ•…éšœæ’é™¤æŒ‡å—

#### ä»»åŠ¡ 4.2ï¼šç›‘æ§ä¸æ—¥å¿—
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§
- [ ] æ·»åŠ é”™è¯¯è¿½è¸ª
- [ ] ä¼˜åŒ–æ—¥å¿—è¾“å‡º

---

## äº”ã€é…ç½®å‚æ•°

### 5.1 æ‰¹å¤„ç†é…ç½®

```rust
// å¯é…ç½®çš„å‚æ•°
pub struct BatchAnalysisConfig {
    pub batch_size: usize,              // æ¯æ‰¹å•è¯æ•°ï¼ˆé»˜è®¤ 10ï¼ŒèŒƒå›´ 5-20ï¼‰
    pub max_concurrent_batches: usize,    // æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼ŒèŒƒå›´ 1-5ï¼‰
    pub retry_failed_words: bool,         // æ˜¯å¦é‡è¯•å¤±è´¥çš„å•è¯ï¼ˆé»˜è®¤ trueï¼‰
    pub max_retries: usize,              // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 2ï¼‰
    pub timeout_per_batch: u64,           // æ¯æ‰¹è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ 60 ç§’ï¼‰
}

impl Default for BatchAnalysisConfig {
    fn default() -> Self {
        Self {
            batch_size: 10,
            max_concurrent_batches: 3,
            retry_failed_words: true,
            max_retries: 2,
            timeout_per_batch: 60,
        }
    }
}
```

### 5.2 æ€§èƒ½è°ƒä¼˜å»ºè®®

**å°è§„æ¨¡ï¼ˆ< 50 å•è¯ï¼‰**
- æ‰¹å¤§å°ï¼š5
- å¹¶å‘æ•°ï¼š2
- ä¼˜å…ˆé€Ÿåº¦

**ä¸­ç­‰è§„æ¨¡ï¼ˆ50-200 å•è¯ï¼‰**
- æ‰¹å¤§å°ï¼š10
- å¹¶å‘æ•°ï¼š3
- å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§

**å¤§è§„æ¨¡ï¼ˆ200-1000 å•è¯ï¼‰**
- æ‰¹å¤§å°ï¼š15
- å¹¶å‘æ•°ï¼š3-4
- ä¼˜å…ˆç¨³å®šæ€§

**è¶…å¤§è§„æ¨¡ï¼ˆ> 1000 å•è¯ï¼‰**
- æ‰¹å¤§å°ï¼š20
- å¹¶å‘æ•°ï¼š4-5
- ä¼˜å…ˆç¨³å®šæ€§

---

## å…­ã€é£é™©è¯„ä¼°ä¸ç¼“è§£

### 6.1 æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| API é™æµ | åˆ†æå˜æ…¢ | ä¸­ | å®ç°æŒ‡æ•°é€€é¿é‡è¯• |
| å†…å­˜å ç”¨é«˜ | åº”ç”¨å´©æºƒ | ä½ | é™åˆ¶å¹¶å‘æ•°ï¼Œä½¿ç”¨æµå¼å¤„ç† |
| Token è¶…é™ | æ‰¹æ¬¡å¤±è´¥ | ä¸­ | åŠ¨æ€è°ƒæ•´æ‰¹å¤§å° |
| ç½‘ç»œä¸ç¨³å®š | æ‰¹æ¬¡è¶…æ—¶ | ä¸­ | å®ç°é‡è¯•æœºåˆ¶ |
| JSON è§£æå¤±è´¥ | å•è¯ä¸¢å¤± | ä½ | å¢å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿— |

### 6.2 å®æ–½é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| å¼€å‘å‘¨æœŸé•¿ | å»¶è¿Ÿä¸Šçº¿ | ä¸­ | åˆ†é˜¶æ®µå®æ–½ï¼Œä¿æŒå‘åå…¼å®¹ |
| å…¼å®¹æ€§é—®é¢˜ | ç°æœ‰åŠŸèƒ½å—å½±å“ | ä½ | ä¿ç•™æ—§ APIï¼Œé€æ­¥è¿ç§» |
| æµ‹è¯•è¦†ç›–ä¸è¶³ | çº¿ä¸Šé—®é¢˜ | ä¸­ | å……åˆ†çš„é›†æˆæµ‹è¯• |
| æ€§èƒ½ä¸è¾¾é¢„æœŸ | ç”¨æˆ·ä½“éªŒå·® | ä½ | æ€§èƒ½æµ‹è¯•ï¼Œå‚æ•°è°ƒä¼˜ |

---

## ä¸ƒã€æˆåŠŸæŒ‡æ ‡

### 7.1 æ€§èƒ½æŒ‡æ ‡

- **å¤„ç†æ—¶é—´**ï¼š500 ä¸ªå•è¯ < 90 ç§’ï¼ˆç›®æ ‡ï¼š< 60 ç§’ï¼‰
- **å¹¶å‘æ•ˆç‡**ï¼š3 ä¸ªå¹¶å‘æ‰¹æ¬¡ï¼ŒCPU åˆ©ç”¨ç‡ > 70%
- **å†…å­˜ä½¿ç”¨**ï¼šå³°å€¼å†…å­˜ < 500MB
- **API è°ƒç”¨æ¬¡æ•°**ï¼š1000 ä¸ªå•è¯ < 100 æ¬¡è°ƒç”¨

### 7.2 å¯é æ€§æŒ‡æ ‡

- **æˆåŠŸç‡**ï¼š> 95% çš„å•è¯æˆåŠŸåˆ†æ
- **æ¢å¤ç‡**ï¼šç½‘ç»œä¸­æ–­å > 90% å·²å®Œæˆæ•°æ®ä¿ç•™
- **è¿›åº¦å‡†ç¡®æ€§**ï¼šè¿›åº¦è¯¯å·® < 5%
- **é”™è¯¯æ¢å¤**ï¼šå¤±è´¥å•è¯è‡ªåŠ¨é‡è¯•æˆåŠŸç‡ > 80%

### 7.3 ç”¨æˆ·ä½“éªŒæŒ‡æ ‡

- **è¿›åº¦å¯è§æ€§**ï¼šç”¨æˆ·èƒ½å®æ—¶çœ‹åˆ°æ¯ä¸ªå•è¯çš„çŠ¶æ€
- **å–æ¶ˆå“åº”æ—¶é—´**ï¼š< 1 ç§’
- **é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦**ï¼šç”¨æˆ·èƒ½ç†è§£é”™è¯¯åŸå› 
- **æ€»ä½“æ»¡æ„åº¦**ï¼šç”¨æˆ·æ»¡æ„åº¦ > 4.5/5.0

---

## å…«ã€é™„å½•

### 8.1 æŠ€æœ¯æ ˆ

- **åç«¯**ï¼šRust, Tauri, tokio, sqlx
- **å‰ç«¯**ï¼šReact, TypeScript, Tauri API
- **AI æœåŠ¡**ï¼šOpenAI å…¼å®¹ APIï¼ˆasync-openaiï¼‰
- **æ•°æ®åº“**ï¼šSQLite

### 8.2 å‚è€ƒèµ„æº

- [Rust å¹¶å‘ç¼–ç¨‹](https://tokio.rs/)
- [Tauri æœ€ä½³å®è·µ](https://tauri.app/v1/guides/)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [æ‰¹é‡å¤„ç†æ¨¡å¼](https://en.wikipedia.org/wiki/Batch_processing)

### 8.3 æœ¯è¯­è¡¨

- **Batchï¼ˆæ‰¹æ¬¡ï¼‰**ï¼šä¸€ç»„åŒæ—¶å¤„ç†çš„å•è¯
- **Concurrentï¼ˆå¹¶å‘ï¼‰**ï¼šåŒæ—¶è¿›è¡Œçš„å¤šä¸ªæ“ä½œ
- **Semaphoreï¼ˆä¿¡å·é‡ï¼‰**ï¼šæ§åˆ¶å¹¶å‘æ•°é‡çš„æœºåˆ¶
- **Streamï¼ˆæµå¼ï¼‰**ï¼šé€æ­¥æ¥æ”¶æ•°æ®çš„æ–¹å¼
- **Extractionï¼ˆæå–ï¼‰**ï¼šä»æ–‡æœ¬ä¸­è·å–å•è¯åˆ—è¡¨
- **Phonicsï¼ˆè‡ªç„¶æ‹¼è¯»ï¼‰**ï¼šè‹±è¯­å‘éŸ³è§„åˆ™ç³»ç»Ÿ

---

## æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡**ä¸‰é˜¶æ®µæ¶æ„**ï¼ˆæå– -> æ‰¹é‡å¹¶è¡Œåˆ†æ -> åˆå¹¶ï¼‰è§£å†³äº†å½“å‰å•ä¸€ LLM è°ƒç”¨çš„æ ¸å¿ƒé—®é¢˜ï¼š

1. **æ€§èƒ½æå‡ 3-5 å€**ï¼šé€šè¿‡æ‰¹é‡å¹¶è¡Œå¤„ç†
2. **æ”¯æŒå¤§è§„æ¨¡æ•°æ®**ï¼šé€šè¿‡åˆ†æ‰¹å¤„ç†çªç ´ token é™åˆ¶
3. **ç»†ç²’åº¦è¿›åº¦**ï¼šæ¯ä¸ªå•è¯éƒ½æœ‰ç‹¬ç«‹çŠ¶æ€
4. **å¼ºå®¹é”™æ€§**ï¼šå•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“
5. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒ 1000+ å•è¯åœºæ™¯

å»ºè®®é‡‡ç”¨**åˆ†é˜¶æ®µå®æ–½**ç­–ç•¥ï¼Œä¼˜å…ˆå®ç°åç«¯åŸºç¡€è®¾æ–½ï¼Œç„¶åæ›´æ–°å‰ç«¯ï¼Œæœ€åè¿›è¡Œå……åˆ†æµ‹è¯•ã€‚
